{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossBee workflow notebook tutorial based on Python script modules for LBD\n",
    "\n",
    "CrossBee is a system that recommends bridging terms (*b-terms*) through an ensemble-based ranking method. It aids experts in uncovering hidden connections between unrelated domains. CrossBee facilitates the ranking, exploration, and investigation of these cross-domain bridging terms. In a broader sense it implements the Swanson's `ABC` model: it looks for $b$-terms that bridge the literature on a selected $a$-term and $c$-term; $b$-terms are the candidates that can possibly explain the link.\n",
    "\n",
    "<hr>\n",
    "\n",
    "[1] Petrič, I., Urbančič, T., Cestnik, B., Macedoni-Lukšič, M. (2009). Literature mining method RaLoLink for uncovering relations between biomedical concepts. Journal of Biomedical Informatics, 42(2), 219–227.\n",
    "\n",
    "[2] Juršič, M., Cestnik, B., Urbančič, T., Lavrač, N. (2012a). Cross-domain Literature Mining: Finding Bridging Concepts with CrossBee, *Proceedings of the 3rd International Conference on Computational Creativity*, 33-40.\n",
    "\n",
    "[3] Juršič, M., Cestnik, B., Urbančič, T., Lavrač, N. (2012b). Bisociative literature mining by ensemble heuristics. In M. R. Berthold (Ed.), *Bisociative Knowledge Discovery*, 338–358. Springer.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Note that **our motive** was to **re-implement parts** of the tools such as **CrossBee**, RaJoLink and OntoGen so that we can generally **repeat the results** with the tools from the past experiments, and not so much to optimize the written Python code. The fucus was on understanding the learning processes and visualizing the workflows in terms of repeatability of the results obtained; the efficiency and elegance of the programming can be addressed in future versions of the scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and initialize `logging` library to track the execution of the scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Initialize logging with a basic configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s: %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LBD components from the framework notebooks. The description of the individual components from the framework notebooks can be found in the respective notebooks.\n",
    "\n",
    "The purpose of the **import_ipynb** library is to allow the direct import of Jupyter notebooks as modules so that code, functions and classes defined in one notebook can be easily reused in other notebooks or Python scripts.\n",
    "If the **import_ipynb** library is omitted (or commented out), the corresponding modules will be imported from **.py** files exported from the **.ipynb** files. Note that importing from **.py** files is usually much faster and therefore more suitable for running scripts in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import import_ipynb\n",
    "import LBD_01_data_acquisition\n",
    "import LBD_02_data_preprocessing\n",
    "import LBD_03_feature_extraction\n",
    "import LBD_04_text_mining\n",
    "import LBD_05_results_analysis\n",
    "import LBD_06_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import additional Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name of the domains $C$ and $A$, then load the responding text from the input file. The expected file format is as follows:\n",
    "\n",
    "1. The file is encoded in Ascii (if it is in UTF-8 or other encoding, it should be converted to Ascii).\n",
    "2. Each line in the file represents one document. The words in each document are separated by spaces. The length of the individual documents may vary.\n",
    "3. The first word in each line is the **unique id**, followed by a semicolon. Normally **pmid** (pubmed id) can be used for this purpose, alhough any unique id (e.g. **sequential count**) suffices.\n",
    "4. The second word in each line can optionally stand for a predefined domain (or class) of the document. In this case, the second word is preceded by **!**. For example, if the file contains documents that originate from two domains, e.g. *migraine* and *magnesium*, the second word in each line is either **!migraine** or **!magnesium**. If the file contains documents that originate from *autism* and *calcineurin*, the second word in each line will be either **!autism** or **!calcineurin**.\n",
    "5. If the second word is not preceded by **!**, it will be considered the first word of the document. In this case, the document will be given the domain **!NA** (**not applicable** or **not available**).\n",
    "\n",
    "\n",
    "**A background story for this experiment**\n",
    "\n",
    "First, we selected *autism* and *calcineurin* as our domains of interest.\n",
    "\n",
    "*Autism*  belongs to a group of pervasive developmental disorders that are portrayed by an\n",
    "early delay and abnormal development of cognitive, communication and social\n",
    "interaction skills of a person. It is a very complex and not yet sufficiently understood\n",
    "domain, where precise causes are still unknown; research suggests that it may be\n",
    "related to genetic mutations, environmental factors, and brain structure and function. \n",
    "*Calcineurin* is a protein phosphatase with a high prevalence in the brain.\n",
    "\n",
    "The dataset from the input file was constructed using the following PubMed query:\n",
    "\n",
    "1. autis* [TIAB] AND 1900/01/01:2007/12/31 [PDAT]\n",
    "2. calcineurin [TIAB] AND 1900/01/01:2007/12/31 [PDAT]\n",
    "\n",
    "The input file *input/f_autism_calcineurin.txt* was prepared for the experiments described in [1]. It contains 13.623 titles and abstracts, 9.403 from *autism* and 4.220 from *calcineurin*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the global variable CONTEXT_SWITCH to the following values, depending on which domain pairs you would like to set as a context:\n",
    "# 1 for Autism-Calcineurin\n",
    "# 2 for Autism-Calcineurin on outlier documents\n",
    "# 11 for Alzheimer-Macrobiota\n",
    "# 12 for Alzheimer-Macrobiota on outlier documents\n",
    "# 21 for Migraine-Magnesium\n",
    "\n",
    "CONTEXT_SWITCH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONTEXT_SWITCH == 1:\n",
    "    domainName = 'Autism-Calcineurin'\n",
    "    fileName = 'input/f_autism_calcineurin.txt'\n",
    "elif CONTEXT_SWITCH == 2:\n",
    "    domainName = 'Autism-Calcineurin'\n",
    "    fileName = 'input/f_autism_calcineurin_outliers.txt'\n",
    "elif CONTEXT_SWITCH == 11:\n",
    "    domainName = 'Alzheimer-Macrobiota'\n",
    "    fileName = 'input/f_alzheimer_gimb.txt'\n",
    "elif CONTEXT_SWITCH == 12:\n",
    "    domainName = 'Alzheimer-Macrobiota'\n",
    "    fileName = 'input/f_alzheimer_gimb_outliers.txt'\n",
    "elif CONTEXT_SWITCH == 21:\n",
    "    domainName = 'Migraine-Magnesium'\n",
    "    fileName = 'input/Magnesium_Migraine_before1988.txt'\n",
    "    \n",
    "lines = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "# display the first 7 lines of the document\n",
    "[LBD_02_data_preprocessing.truncate_with_ellipsis(line, 110) for line in lines[:7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display a few middle lines of the document\n",
    "[LBD_02_data_preprocessing.truncate_with_ellipsis(line, 110) for line in lines[9400:9407]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display the last lines of the document\n",
    "[LBD_02_data_preprocessing.truncate_with_ellipsis(line, 110) for line in lines[-7:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess the documents into a dictionary - might take a few minutes for longer files**\n",
    "\n",
    "The script in the next cell is used to prepare text data for further analysis in Literature-Based Discovery (LBD). The aim is to clean, standardize and structure the documents so that they are suitable for further tasks such as feature extraction, topic modeling and the discovery of hidden relationships in the literature. The script prepares the documents stored in `lines` in a dictionary.\n",
    "\n",
    "<details>\n",
    "  <summary>Click for more ...</summary>\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Creating a dictionary from raw data*: The script starts by converting a list of rows into a structured dictionary. \n",
    "    - *`construct_dict_from_list`*: this function takes the raw list of text lines (`lines`) and creates a dictionary (`docs_dict`) in which each entry typically represents a document, with a unique identifier as the key and the text of the document as the value.\n",
    "    - This conversion is important because it puts the text data into a more manageable format that allows efficient processing and retrieval.\n",
    "\n",
    "2. *Preprocessing of documents*: the script then applies various pre-processing steps to the documents:\n",
    "    - *Cleaning*: the text is cleaned to remove unwanted characters, punctuation and other errors.\n",
    "    - *Remove stop words*: frequent words that do not provide meaningful information (e.g. \"the\", \"and\") are removed.\n",
    "    - *Lemmatization*: words are reduced to their base or root form (e.g. \"running\" becomes \"run\") to ensure consistency.\n",
    "    - *Minimum word length*: words shorter than four characters are filtered out.\n",
    "    - *Keep only nouns*: the parameter `keep_only_nouns=True` ensures that only nouns (and proper nouns) are considered in further analysis, removing other word types like adjactives and verbs. \n",
    "    A trained pipeline from *spacy* named *en_core_web_md* is used for the task [https://spacy.io/models/en#en_core_web_md]. Note that this filter uses functions from an external *spacy* library to check every word in the vocabulary of the documents and is therefore time-consuming.\n",
    "    - *MeSH-specific filtering*: the parameter `keep_only_mesh=False` skips the MeSH filtering in this preprocessing.\n",
    "\n",
    "3. *Extract document IDs and processed text*:  the script then extracts lists of document IDs and the corresponding preprocessed text:\n",
    "    - *`extract_ids_list`*: returns a list of document IDs from the preprocessed dictionary to facilitate document lookup and management.\n",
    "    - *`extract_preprocessed_documents_list`*: extracts the cleaned and processed text for each document to prepare it for feature extraction or other analysis.\n",
    "By extracting these lists, the script organizes the data in a format that is easy to manipulate in subsequent steps, such as creating a Bag of Words (BoW) model or calculating TF-IDF scores.\n",
    "\n",
    "**Practical applications**\n",
    "\n",
    "- *Biomedical research and discovery*: This pre-processing approach is valuable in the biomedical field, where ensuring the relevance and accuracy of terms is critical to discovering new relationships between diseases, drugs and other biological concepts. By focusing on specific vocabularies such as MeSH, researchers can more effectively search the literature for new hypotheses or overlooked relationships.\n",
    "- *Data preparation for machine learning*: The cleaned and structured data generated by this script can be fed directly into machine learning models for tasks such as document classification or clustering.\n",
    "\n",
    "**Use**\n",
    "\n",
    "To use this script effectively:\n",
    "1. *Prepare the data*: Make sure you have a list of raw text lines (`lines`).\n",
    "2. *Execute the preprocessing steps*: Run the script to clean, filter and structure the text data.\n",
    "3. *Extract and analyze*: Use the extracted IDs and processed text for further analysis, e.g. to create models and visualizations or for exploratory research.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Creating a dictionary from raw data\n",
    "docs_dict = LBD_02_data_preprocessing.construct_dict_from_list(lines)\n",
    "\n",
    "# 2. Preprocessing of documents\n",
    "keep_list = []\n",
    "# Normally, the original domain names are removed from the vocabulary. If \"migraine\" and \"magnesium\" were added to the vocabulary during the comparison, \n",
    "# they might dominate the analysis because they are the focus of the study. The alorithms might attach extra importance to these terms \n",
    "# and push less obvious but potentially important terms or concepts into the background.\n",
    "\n",
    "if (CONTEXT_SWITCH == 1) or (CONTEXT_SWITCH == 2):\n",
    "    remove_list = ['autism', 'calcineurin']\n",
    "    min_word_length = 4\n",
    "elif (CONTEXT_SWITCH == 11) or (CONTEXT_SWITCH == 12):\n",
    "    remove_list = ['alzheimer', 'gut', 'microbiota']\n",
    "    min_word_length = 5\n",
    "elif (CONTEXT_SWITCH == 21):\n",
    "    keep_list = [\"p\"]\n",
    "    remove_list = [\"migraine\", \"magnesium\"]\n",
    "    min_word_length = 0\n",
    "else:\n",
    "    remove_list = []\n",
    "    min_word_length = 0\n",
    "\n",
    "prep_docs_dict = LBD_02_data_preprocessing.preprocess_docs_dict(\n",
    "    docs_dict, keep_list = keep_list, remove_list = remove_list, mesh_word_list = [], \\\n",
    "    cleaning = True, remove_stopwords = True, lemmatization = True, \\\n",
    "    min_word_length = min_word_length, keep_only_nouns = False, keep_only_mesh = False, stemming = False, stem_type = None)\n",
    "\n",
    "# 3. Extract document IDs and processed text\n",
    "ids_list = LBD_02_data_preprocessing.extract_ids_list(prep_docs_dict)\n",
    "prep_docs_list = LBD_02_data_preprocessing.extract_preprocessed_documents_list(prep_docs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three cells show the first dictionary entries, the document IDs (PubMed or a counter) and the pre-processed documents.\n",
    "\n",
    "When displaying the first few dictionary entries, we can observe the difference between the original and the pre-processed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display the first 7 dictionary items\n",
    "truncated_dict = {\n",
    "    key: {sub_key: LBD_02_data_preprocessing.truncate_with_ellipsis(value, 110) for sub_key, value in sub_dict.items()}\n",
    "    for key, sub_dict in itertools.islice(prep_docs_dict.items(), 7)\n",
    "}\n",
    "truncated_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display the ids of the first 7 documents\n",
    "ids_list[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display the preprocessed text for the first 7 documents\n",
    "[LBD_02_data_preprocessing.truncate_with_ellipsis(line, 110) for line in prep_docs_list[:7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oprionally: open the file in write mode ('w') and write each string to the file\n",
    "# with open('prep_lines.txt', 'w', encoding=\"ascii\") as file:\n",
    "#     for line in prep_docs_list:\n",
    "#         file.write(line + '\\n')  # Add a newline character after each string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct BoW model from important words and n-grams**\n",
    "\n",
    "The next script continues the feature extraction process and focuses on refining a Bag of Words (BoW) model by filtering out less important terms and n-grams. It creates a Bag of Words matrix from the list of pre-processed documents. It then removes n-gram words that occur less than *min_ngram_count* times (in our case 3) in the entire document corpus. The words that are not contained in the MESH list *mesh_word_list* are also removed. This step is important to improve the quality and relevance of the text representation by reducing the vocabulary so that the following steps can be carried out more efficiently (in terms of time).\n",
    "\n",
    "<details>\n",
    "  <summary>Click for more ...</summary>\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Set parameters*: The script starts by setting the parameters for the n-gram size and the minimum document frequency:\n",
    "    - *`ngram_size`*: Specifies that the model considers pairs of consecutive words (bigrams) as features.\n",
    "    - *`min_df`*: Specifies the minimum number of documents in which a word or n-gram must occur in order to be included in the initial vocabulary.\n",
    "\n",
    "2. *Create Bag of Words representation*: The next step is to create the BoW model using the specified n-gram size.\n",
    "This function creates a vocabulary (`word_list`) from all terms and n-grams found in the preprocessed documents (`prep_docs_list`), together with the corresponding frequency matrix (`bow_matrix`). The output vocabulary includes all n-grams without filtering.\n",
    "\n",
    "3. *Filtering low-frequency n-grams*: The script then filters out n-grams that occur less frequently than a certain threshold:\n",
    "    - *`min_count_ngram`*: Specifies the minimum number of occurrences of n-grams to keep.\n",
    "    - The script calculates two important metrics:\n",
    "        - *document frequency*: How many documents contain each word or n-gram.\n",
    "        - *total frequency*: How often each word or n-gram appears in all documents.\n",
    "\n",
    "4. *Filtering based on specific criteria*: The script applies a more sophisticated filtering process to refine the vocabulary. The loop evaluates each term or n-gram in the vocabulary:\n",
    "   - *Non-n-grams*: Will only be retained if they are in a predefined `mesh_word_list`.\n",
    "   - *n-grams*: Are retained if:\n",
    "       - They fulfill the minimum frequency criteria.\n",
    "       - All partial words are contained in `mesh_word_list`.\n",
    "       - The n-gram does not consist of repeated words (e.g. \"word word\").\n",
    "\n",
    "\n",
    "5. *Applying the filters*: The script then filters both the rows and the columns of the BoW matrix. \n",
    "   - *`filter_matrix_columns`*: Refines the BoW matrix by retaining only the selected words or n-grams that meet the filter criteria.\n",
    "   - The updated vocabulary and matrix are then stored in `word_list` and `bow_matrix`, respectively.\n",
    "\n",
    "**Practical applications**\n",
    "\n",
    "- *Biomedical research and discovery*: This filtering method is particularly useful in medical research, where the focus is on extracting and analyzing relevant biomedical terms and concepts.\n",
    "- *Document analysis and classification*: By refining the feature set, this script can improve the performance of classifiers used in the categorization of scientific literature or other text corpora.\n",
    "- *Network analysis*: The filtered vocabulary can serve as a node in a network graph representing meaningful terms and their co-occurrence, which can be analyzed to detect hidden connections.\n",
    "\n",
    "**Use**\n",
    "\n",
    "To use this script effectively, you need to make sure you have a preprocessed document list (`prep_docs_list`). Adjust the parameters like `ngram_size`, `min_df` and `min_count_ngram` to your specific needs. After running the script, you will get a filtered vocabulary and a corresponding BoW matrix, which is more suitable for further analysis such as clustering, topic modeling or discovering new hypotheses in biomedical research.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Set parameters\n",
    "if (CONTEXT_SWITCH == 1) or (CONTEXT_SWITCH == 2):\n",
    "    ngram_size = 2 # to reduce the vocabulary, only single words and bigrams are used for further analysis\n",
    "    min_df = 2 # single words and bigrams should appear in the documents at least two times to be used for further analysis \n",
    "    max_features = 30000\n",
    "elif (CONTEXT_SWITCH == 11) or (CONTEXT_SWITCH == 12):\n",
    "    ngram_size = 3\n",
    "    min_df = 1\n",
    "    max_features = None\n",
    "elif (CONTEXT_SWITCH == 21):\n",
    "    ngram_size = 3\n",
    "    min_df = 1\n",
    "    max_features = None\n",
    "\n",
    "# 2. Create Bag of Words representation\n",
    "word_list, bow_matrix = LBD_03_feature_extraction.create_bag_of_words(prep_docs_list, ngram_size, min_df, max_features=max_features)\n",
    "print('Number of terms in initial vocabulary with all n-grams: ', len(word_list))\n",
    "\n",
    "# 3. Filtering low-frequency n-grams\n",
    "#    remove n-grams with frequency count less than min_count_ngram from vocabulary word_list and bow_matrix\n",
    "if (CONTEXT_SWITCH == 1) or (CONTEXT_SWITCH == 2):\n",
    "    min_count_ngram = 3\n",
    "elif (CONTEXT_SWITCH == 11) or (CONTEXT_SWITCH == 12):\n",
    "    min_count_ngram = 3\n",
    "elif (CONTEXT_SWITCH == 21):\n",
    "    min_count_ngram = 2\n",
    "\n",
    "tmp_sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "tmp_sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "# 4. Filtering based on specific criteria\n",
    "tmp_filter_columns = []\n",
    "for i, word in enumerate(word_list):\n",
    "    if not LBD_03_feature_extraction.word_is_nterm(word):\n",
    "        tmp_filter_columns.append(i)\n",
    "    else:\n",
    "        if tmp_sum_count_word_in_docs[word] >= min_count_ngram:\n",
    "            tmp_filter_columns.append(i)\n",
    "\n",
    "# 5. Applying the filters\n",
    "#    keep the original order of rows\n",
    "tmp_filter_rows = []\n",
    "for i, id in enumerate(ids_list):\n",
    "    tmp_filter_rows.append(i)\n",
    "\n",
    "tmp_filtered_word_list, tmp_filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, bow_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "bow_matrix = tmp_filtered_bow_matrix\n",
    "print('Number of terms in preprocessed vocabulary after removing infrequent n-grams: ', len(word_list))\n",
    "\n",
    "# Output the lists for checking the order\n",
    "#LBD_02_data_preprocessing.save_list_to_file(word_list, \"output/_list.txt\")\n",
    "#LBD_02_data_preprocessing.save_list_to_file(prep_docs_list, \"output/_prep_list.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate relevant indicators for the BoW matrix**\n",
    "\n",
    "The script in the next cell is a continuation of the text preprocessing pipeline that calculates the margins for the Bag of Words (BoW) matrix and optimizes the BoW matrix for better interpretability and analysis. By arranging the matrix to highlight the most important terms and documents, this script helps to recognize patterns in the data, which is a crucial step in LBD.\n",
    "\n",
    "<details>\n",
    "  <summary>Click for more ...</summary>\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Counting word frequencies*: The script begins by calculating various frequency counts that provide insight into how words are distributed across documents:\n",
    "   - *`sum_count_docs_containing_word`*: counts how many documents each word appears in\n",
    "   - *`sum_count_word_in_docs`*: counts the occurrences of each word across all documents\n",
    "   - *`sum_count_words_in_doc`*: counts the total number of words in each document\n",
    "\n",
    "   These metrics are essential for understanding the significance and distribution of terms within the corpus, which can guide further analysis.\n",
    "\n",
    "2. *Displaying frequency counts*: The script then prints a subset of these frequency counts to give an overview of the data:\n",
    "   - *`islice`* from `itertools` is used to print just the first few items, making it easier to inspect the data without overwhelming output.\n",
    "   - These print statements help users quickly assess the distribution and frequency of words and documents in the BoW model.\n",
    "\n",
    "3. *Optimizing the BoW matrix*: The script proceeds to rearrange the BoW matrix so that the most frequent words and documents are positioned at the top-left corner of the matrix:\n",
    "   - *sorting*: The words and documents are sorted by their frequencies in descending order.\n",
    "   - *filtering*: The indices of these sorted words and documents are then used to rearrange the BoW matrix.\n",
    "\n",
    "   This step ensures that the most significant terms and documents are easily accessible, facilitating further analysis such as clustering, topic modeling, or visualization.\n",
    "\n",
    "4. *Rearranging the matrix*: Finally, the script filters the matrix according to the computed order:\n",
    "   - *`filter_matrix`*: This function reorders the BoW matrix based on the sorted indices, ensuring that the most relevant terms and documents are emphasized.\n",
    "\n",
    "   The script then prints out the first few items in the reordered lists:\n",
    "   - This output allows users to verify that the matrix has been rearranged as intended, highlighting the most important elements of the dataset.\n",
    "\n",
    "**Use**\n",
    "\n",
    "To use this script, you must have a BoW matrix (`bow_matrix`) and the corresponding lists of words (`word_list`) and document IDs (`ids_list`). The script processes these inputs to calculate the frequency counts, reorder the matrix and output the reordered BoW matrix. This optimized matrix can be used for various downstream tasks, e.g. for creating visualizations, for deeper statistical analysis or as a basis for machine learning models for predictions.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Counting word frequencies\n",
    "sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "sum_count_words_in_doc = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, bow_matrix)\n",
    "\n",
    "# 2. Displaying frequency counts\n",
    "print('Number of documents in which each word is present: ', dict(itertools.islice(sum_count_docs_containing_word.items(), 7)))\n",
    "print('Number of occurences of each word in all documents: ', dict(itertools.islice(sum_count_word_in_docs.items(), 7)))\n",
    "print('Number of words in each document: ', dict(itertools.islice(sum_count_words_in_doc.items(), 7)))\n",
    "\n",
    "# 3. Optimizing the BoW matrix\n",
    "#    Compute the order of rows (documents) and columns (words) in the bow matrix so that the most frequent words are in the top-left corner. \n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_word_in_docs, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_words_in_doc, reverse=True), ids_list) \n",
    "\n",
    "# 4. Rearranging the matrix\n",
    "#    Rearange (filter) the BoW matrix according to the previously computed order.\n",
    "filtered_ids_list, filtered_word_list, filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, bow_matrix, filter_rows, filter_columns)\n",
    "\n",
    "print('The first few documents in the rows of the filtered bow matrix: ', filtered_ids_list[:7])\n",
    "print('The first few words in the columns of the filtered bow matrix: ', filtered_word_list[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize a part of BoW matrix**\n",
    "\n",
    "Visualize the upper left part of the Bag of Words (BoW) matrix. In the BoW matrix, each row corresponds to a document and each column to a word (or n-gram). The values in the matrix represent the frequency of the word in the corresponding document.\n",
    "As the BoW matrix mainly contains zeros, the displayed matrix is sorted so that the higher values in the cells are moved to the top left-hand corner of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 15\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered Bag of Words', \\\n",
    "                                           filtered_bow_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], \\\n",
    "                                           filtered_word_list[first_column:last_column], as_int = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct TF-IDF matrix from important words and n-grams**\n",
    "\n",
    "The next script is designed to create a Term Frequency-Inverse Document Frequency (TF-IDF) matrix from a set of preprocessed documents and then refine this matrix by filtering out less relevant terms.\n",
    "\n",
    "<details>\n",
    "  <summary>Click for more ...</summary>\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Creating the TF-IDF matrix*:<br>\n",
    "   The script begins by generating a TF-IDF matrix using a list of preprocessed documents:\n",
    "   - *TF-IDF matrix*: This matrix represents the importance of each word (or n-gram) across all documents in the corpus.\n",
    "   - *`ngram_size`*: Specifies the size of word sequences to consider (e.g., unigrams, bigrams).\n",
    "   - *`min_df`*: Filters out terms that appear in fewer than a specified number of documents, reducing noise in the analysis.\n",
    "\n",
    "   This step is essential for transforming raw text data into a structured format that highlights important terms.\n",
    "\n",
    "2. *Rearranging the TF-IDF matrix*:\n",
    "   The script then refines the TF-IDF matrix by rearranging and filtering the terms:\n",
    "   - *filtering*: The matrix is filtered based on criteria such as the importance of terms, ensuring that only the most relevant words remain.\n",
    "   - *rearranging*: The matrix is reorganized according to a predefined order, based on the significance of terms or their relevance to specific documents.\n",
    "\n",
    "   This refinement process is crucial for improving the quality of the analysis by focusing on the most impactful terms, which can lead to more accurate and insightful results.\n",
    "\n",
    "**Use**\n",
    "\n",
    "Users can apply this script as part of a larger text mining workflow where the TF-IDF matrix serves as an important step in structuring and analyzing the data. By filtering and refining the matrix, users can ensure that their analysis focuses on the most relevant and meaningful terms, leading to more meaningful insights. In the context of LBD, this script is an essential tool for turning raw text data into actionable insights.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Creating the TF-IDF matrix\n",
    "word_list, tfidf_matrix = LBD_03_feature_extraction.create_tfidf(prep_docs_list, ngram_size, min_df, max_features=max_features)\n",
    "print('Number of terms in initial vocabulary with all n-grams: ', len(word_list))\n",
    "\n",
    "# 2. Rearranging the TF-IDF matrix\n",
    "#    Rearange (filter) the TF-IDF matrix according to the previously computed order from bow matrix.\n",
    "tmp_filtered_word_list, tmp_filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, tfidf_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "tfidf_matrix = tmp_filtered_tfidf_matrix\n",
    "print('Number of terms in preprocessed vocabulary after removing infrequent n-grams: ', len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute margins for TF-IDF matrix**\n",
    "\n",
    "This script is designed to analyze and manipulate Term Frequency-Inverse Document Frequency (TF-IDF) data for a corpus of documents. It computes various statistics related to the TF-IDF values for both words and documents, then filters the TF-IDF matrix to reorder it based on the most important words and documents.\n",
    "\n",
    "<details>\n",
    "  <summary>Click for more ...</summary>\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Summing and maximizing TF-IDF values*:\n",
    "   - `sum_count_each_word_in_all_documents`: calculates the sum of TF-IDF scores for each word across all documents, providing insight into the overall importance of words in the entire corpus\n",
    "   - `max_tfidf_each_word_in_all_documents`: finds the maximum TF-IDF score for each word, indicating the document where each word is most important\n",
    "   - `sum_count_all_words_in_each_document`: computes the sum of TF-IDF scores for all words in each document, which can be used to determine the \"weight\" or importance of the document itself\n",
    "   - `max_tfidf_all_words_in_each_document`: identifies the highest TF-IDF score for each document, which can help isolate which document contains particularly important terms\n",
    "\n",
    "2. *Output statistics*:\n",
    "   - The script uses Python's `itertools.islice` function to print a preview of the top 7 values from each TF-IDF statistic. This offers a quick way to inspect the data without overwhelming the output with large lists.\n",
    "\n",
    "3. *Sorting and filtering the TF-IDF matrix*:\n",
    "   - After calculating the TF-IDF statistics, the script computes an ordering for the rows (documents) and columns (words) based on the maximum TF-IDF values. This ensures that the most important terms and documents are given priority in subsequent analyses.\n",
    "   - The `filter_matrix` function then reorders the original TF-IDF matrix based on these computed rankings, allowing for a focused view of the most significant content in the corpus.\n",
    "\n",
    "**Practical applications**\n",
    "\n",
    "- *Document analysis and classification*: By identifying the most important terms and documents in a corpus, this technique can assist in classifying documents into relevant categories.\n",
    "- *Term and key concept extraction*: Researchers can use the sum and max TF-IDF scores to isolate critical keywords that may represent novel concepts or ideas in the context of Literature-Based Discovery.\n",
    "- *Summarization and information retrieval*: By filtering out less important words and documents, this script can help narrow down a large corpus to the most relevant data, making retrieval tasks more efficient.\n",
    "\n",
    "**Use**\n",
    "\n",
    "This script is a practical tool for analyzing TF-IDF data in text mining applications. By summing and maximizing TF-IDF scores for words and documents, users can highlight the most significant elements of their corpus. The filtered matrix provides a more focused view of the most important terms, which is highly useful in fields like Literature-Based Discovery and NLP.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Summing and maximizing TF-IDF values\n",
    "sum_word_tfidf = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "max_word_tfidf = LBD_03_feature_extraction.max_tfidf_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "\n",
    "sum_doc_tfidf = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "max_doc_tfidf = LBD_03_feature_extraction.max_tfidf_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "\n",
    "# 2. Output statistics\n",
    "print('Sum of TF-IDF for each word: ', dict(itertools.islice(sum_word_tfidf.items(), 7)))\n",
    "print('Max of TF-IDF for each word: ', dict(itertools.islice(max_word_tfidf.items(), 7)))\n",
    "\n",
    "print('Sum of TF-IDF for each document: ', dict(itertools.islice(sum_doc_tfidf.items(), 7)))\n",
    "print('Max of TF-IDF for each document: ', dict(itertools.islice(max_doc_tfidf.items(), 7)))\n",
    "\n",
    "# 3. Sorting and filtering the TF-IDF matrix\n",
    "#    Compute the order of rows (documents) and columns (words) in the tfidf matrix so that the most important words are in the top-left corner. \n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_doc_tfidf, reverse=True), ids_list) \n",
    "\n",
    "#    Rearange (filter) the TF-IDF matrix according to the previously computed order.\n",
    "filtered_ids_list, filtered_word_list, filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, tfidf_matrix, filter_rows, filter_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize a part of TF-IDF matrix**\n",
    "\n",
    "Visualize the upper left part of the TF-IDF matrix. In the TF-IDF matrix, each row corresponds to a document and each column to a word (or n-gram). The values in the matrix represent the Term Frequency Inverse Document Frequency (abbreviated TF-IDF) of the word (term) in the corresponding document and document corpus. TF-IDF is a measure of how relevant a word in a document is in relation to a corpus: the measure increases proportionally to the number of occurrences of a word in the text, but is compensated for by the word frequency in the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 25\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered TfIdf', filtered_tfidf_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], filtered_word_list[first_column:last_column], as_int = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of domain names of all documents (from the dictionary containing the documents) and a list of unique domain names. In the first context (CONTEXT_SWITCH = 1 or 2) there are two distinct domains: *Autism* and *Calcineurin*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "domains_list = LBD_02_data_preprocessing.extract_domain_names_list(prep_docs_dict)\n",
    "print('Domain names for the first few documents: ', domains_list[:7])\n",
    "unique_domains_list = LBD_02_data_preprocessing.extract_unique_domain_names_list(prep_docs_dict)\n",
    "print('A list of all uniques domain names in all the documents: ', unique_domains_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find outlier documents for *outlier-based heuristics* calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Matrix rows, columns: ', tfidf_matrix.shape) # rows, columns\n",
    "tfidf_matrix = np.asarray(tfidf_matrix)\n",
    "\n",
    "# Find outlier documents\n",
    "outlier_docs = LBD_04_text_mining.find_document_outliers(tfidf_matrix,  n_outliers=20)\n",
    "print('Outlier docs indeces: ', outlier_docs) # list of documents is lines and preprocessed in prep_docs_list; ids_list contains document ids\n",
    "outlier_docs_list = outlier_docs.astype(int).tolist()\n",
    "print('Outlier docs ids: ', [ids_list[i] for i in outlier_docs_list])\n",
    "\n",
    "# Find representative documents\n",
    "representative_docs = LBD_04_text_mining.find_document_outliers(tfidf_matrix,  n_outliers=-20)\n",
    "print('Representative docs indices: ', representative_docs) # list of documents is lines and preprocessed in prep_docs_list; ids_list contains document ids\n",
    "representative_docs_list = representative_docs.astype(int).tolist()\n",
    "print('Representative docs ids: ', [ids_list[i] for i in representative_docs_list])\n",
    "\n",
    "# Find outlier words\n",
    "outlier_words = LBD_04_text_mining.find_word_outliers(tfidf_matrix, word_list,  n_outliers=20)\n",
    "print('Outlier words: ', outlier_words)\n",
    "\n",
    "# Find representative words\n",
    "representative_words = LBD_04_text_mining.find_word_outliers(tfidf_matrix, word_list,  n_outliers=-20)\n",
    "print('Representative words: ', representative_words)\n",
    "\n",
    "\n",
    "# Find again outlier documents, this time with the proper size for furter processing\n",
    "outlier_docs = LBD_04_text_mining.find_document_outliers(tfidf_matrix,  n_outliers=3000)\n",
    "outlier_docs_list = outlier_docs.astype(int).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregating Bag-of-Words matrix by domain**\n",
    "\n",
    "This script helps users aggregate a Bag-of-Words (BoW) matrix based on domain information, which collapses individual document representations into aggregated domain-level representations, allowing for easier analysis of textual data categorized by domains.\n",
    "\n",
    "<details>\n",
    "  <summary>Click for more ...</summary>\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "This script processes a Bag-of-Words matrix (`bow_matrix`), grouping documents by their associated domain names from a `domains_list`. Each unique domain name in `unique_domains_list` will have all its corresponding documents' BoW vectors summed into a single domain-level vector. Here's a step-by-step breakdown:\n",
    "\n",
    "1. *Create binary martix from BoW matrix*:  \n",
    "   `binary_matrix = (bow_matrix > 0).astype(int)` creates binary matrix contaiong only 0 and 1 from the BoW matrix; the binary matrix is used to count number of documents that contain a word.\n",
    "\n",
    "2. *Initialize empty matrix*:  \n",
    "   `domains_bow_matrix = np.empty((0, bow_matrix.shape[1]))` initializes an empty matrix that will store the aggregated BoW vectors for each domain.\n",
    "\n",
    "3. *Count documents per domain*:  \n",
    "   The dictionary `no_documents_in_domain = {}` is created to track how many documents belong to each domain. This is useful for understanding the distribution of documents across domains.\n",
    "\n",
    "4. *Iterate through domains*:  \n",
    "   The script iterates over each unique domain in `unique_domains_list`. For each domain:\n",
    "   - *Find domain documents*:  \n",
    "     `domain_docs_indices` stores the indices of all documents in `domains_list` that belong to the current domain.\n",
    "   - *Count and record document count*:  \n",
    "     The length of `domain_docs_indices` is stored in the dictionary `no_documents_in_domain[domain_name]`.\n",
    "   - *Aggregate BoW vectors*:  \n",
    "     Using the document indices, the script selects the corresponding rows from `bow_matrix`, sums them (`tmp = (bow_matrix[domain_docs_indices,:]).sum(axis=0)`), and appends the result to `domains_bow_matrix`.\n",
    "\n",
    "5. *Output*:  \n",
    "   After processing all domains, the script prints two outputs:\n",
    "   - `domains_bow_matrix`: The BoW matrix, now aggregated by domains.\n",
    "   - `no_documents_in_domain`: A dictionary showing how many documents were grouped under each domain.\n",
    "\n",
    "**Practical Applications**\n",
    "\n",
    "This script is highly useful in various NLP and text mining fields, particularly when dealing with large collections of text that can be grouped into domains or categories:\n",
    "\n",
    "- *Biomedical research and discovery*: Researchers can use the aggregated domain-level BoW vectors to study relationships between different knowledge domains and detect novel hypotheses or cross-disciplinary insights.\n",
    "- *Topic modeling and clustering*: Aggregating BoW vectors by domain enables better analysis of domain-specific topics, which could be useful in extracting themes from large bodies of literature. Grouping documents by domain can facilitate clustering, allowing users to investigate how text clusters vary across different domains.\n",
    "\n",
    "**Use**\n",
    "\n",
    "To use this script:\n",
    "1. *Prepare Input*: Ensure you have a Bag-of-Words matrix (`bow_matrix`), where each row represents a document's word vector. You also need a list of domains (`domains_list`) with the same length as the number of documents, and a list of unique domain names (`unique_domains_list`).\n",
    "2. *Run the Script*: Execute the script in a Python environment (after importing `numpy` as `np`), and it will return two key outputs:\n",
    "   - `domains_bow_matrix`: The BoW vectors aggregated by domain.\n",
    "   - `no_documents_in_domain`: The count of documents per domain.\n",
    "   \n",
    "These outputs can be used in further analysis, such as domain-specific topic modeling, clustering, or visualization tasks. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Create binary martix from BoW matrix\n",
    "binary_matrix = (bow_matrix > 0).astype(int)\n",
    "\n",
    "# 2. Initialize empty matrices\n",
    "domains_bow_matrix = np.empty((0, bow_matrix.shape[1]))\n",
    "domains_binary_matrix = np.empty((0, binary_matrix.shape[1]))\n",
    "\n",
    "domains_outlier_bow_matrix = np.empty((0, bow_matrix.shape[1]))\n",
    "domains_outlier_binary_matrix = np.empty((0, binary_matrix.shape[1]))\n",
    "\n",
    "domains_sum_tfidf_matrix = np.empty((0, tfidf_matrix.shape[1]))\n",
    "domains_max_tfidf_matrix = np.empty((0, tfidf_matrix.shape[1]))\n",
    "\n",
    "domains_outlier_sum_tfidf_matrix = np.empty((0, tfidf_matrix.shape[1]))\n",
    "domains_outlier_max_tfidf_matrix = np.empty((0, tfidf_matrix.shape[1]))\n",
    "\n",
    "# 3. Count documents per domain\n",
    "no_documents_in_domain = {}\n",
    "no_outlier_documents_in_domain = {}\n",
    "\n",
    "# 4. Iterate through domains\n",
    "for i, domain_name in enumerate(unique_domains_list):\n",
    "    domain_docs_indices = [i for i, label in enumerate(domains_list) if label == domain_name]\n",
    "    no_documents_in_domain[domain_name] = len(domain_docs_indices)\n",
    "\n",
    "    domain_outlier_docs_indices = [i for i, label in enumerate(domains_list) if (label == domain_name) and (i in outlier_docs)]\n",
    "    no_outlier_documents_in_domain[domain_name] = len(domain_outlier_docs_indices)\n",
    "\n",
    "    tmp0 = (bow_matrix[domain_docs_indices,:]).sum(axis=0)\n",
    "    domains_bow_matrix = np.vstack((domains_bow_matrix, tmp0))\n",
    "\n",
    "    tmp1 = (binary_matrix[domain_docs_indices,:]).sum(axis=0)\n",
    "    domains_binary_matrix = np.vstack((domains_binary_matrix, tmp1))\n",
    "\n",
    "    tmp2 = (bow_matrix[domain_outlier_docs_indices,:]).sum(axis=0)\n",
    "    domains_outlier_bow_matrix = np.vstack((domains_outlier_bow_matrix, tmp2))\n",
    "\n",
    "    tmp3 = (binary_matrix[domain_outlier_docs_indices,:]).sum(axis=0)\n",
    "    domains_outlier_binary_matrix = np.vstack((domains_outlier_binary_matrix, tmp3))\n",
    "\n",
    "\n",
    "    tmp4 = (tfidf_matrix[domain_docs_indices,:]).sum(axis=0)\n",
    "    domains_sum_tfidf_matrix = np.vstack((domains_sum_tfidf_matrix, tmp4))\n",
    "\n",
    "    tmp5 = (tfidf_matrix[domain_docs_indices,:]).max(axis=0)\n",
    "    domains_max_tfidf_matrix = np.vstack((domains_max_tfidf_matrix, tmp5))\n",
    "\n",
    "    tmp6 = (tfidf_matrix[domain_outlier_docs_indices,:]).sum(axis=0)\n",
    "    domains_outlier_sum_tfidf_matrix = np.vstack((domains_outlier_sum_tfidf_matrix, tmp6))\n",
    "\n",
    "    tmp7 = (tfidf_matrix[domain_outlier_docs_indices,:]).max(axis=0)\n",
    "    domains_outlier_max_tfidf_matrix = np.vstack((domains_outlier_max_tfidf_matrix, tmp7))\n",
    "\n",
    "# 5. Output\n",
    "print('Aggregated BoW matrix for the domains:')\n",
    "print(domains_bow_matrix)\n",
    "print('Aggregated binary matrix for the domains:')\n",
    "print(domains_binary_matrix)\n",
    "print('Aggregated outliers BoW matrix for the domains:')\n",
    "print(domains_outlier_bow_matrix)\n",
    "print('Aggregated outliers binary matrix for the domains:')\n",
    "print(domains_outlier_binary_matrix)\n",
    "print()\n",
    "print('Aggregated sum TF-IDF matrix for the domains:')\n",
    "print(domains_sum_tfidf_matrix)\n",
    "print('Aggregated max TF-IDF matrix for the domains:')\n",
    "print(domains_max_tfidf_matrix)\n",
    "print('Aggregated outliers sum TF-IDF matrix for the domains:')\n",
    "print(domains_outlier_sum_tfidf_matrix)\n",
    "print('Aggregated outliers max TF-IDF matrix for the domains:')\n",
    "print(domains_outlier_max_tfidf_matrix)\n",
    "print()\n",
    "print('Number of documents in each domain: ', no_documents_in_domain)\n",
    "print('Number of outlier documents in each domain: ', no_outlier_documents_in_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created eight matrices that have two rows (one for each domain) and *len(word_list)* columns (one for each word in the vocabulary):\n",
    "\n",
    "1. domains_bow_matrix: frequency of words in domains (domain documents)\n",
    "2. domains_binary_matrix: frequency of documents containing word in domains (each word is counted as 1 if present in a docuemnt)\n",
    "3. domains_outlier_bow_matrix: in outlier documents only, frequency of words in domains (domain documents)\n",
    "4. domains_outlier_binary_matrix: in outlier documents only, frequency of documents containing word in domains\n",
    "5. domains_sum_tfidf_matrix: for each word, sum of TF-IDFs in domains \n",
    "6. domains_max_tfidf_matrix: for each word, max of TF-IDFs in domains\n",
    "7. domains_outlier_sum_tfidf_matrix: in outlier documents only, for each word, sum of TF-IDFs in domains\n",
    "8. domains_outlier_max_tfidf_matrix: in outlier documents only, for each word, max of TF-IDFs in domains\n",
    "\n",
    "These matrices are used to compute heuristics defined in the following cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieving word frequency for a specific domain in a matrix**\n",
    "\n",
    "This function, `cell_value_in_a_matrix()`, allows users to easily extract the frequency of a specific word within a particular domain from an aggregated matrix (BoW or TF-IDF).\n",
    "\n",
    "<details>\n",
    "  <summary>Click for more ...</summary>\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "The `cell_value_in_a_matrix()` function takes three inputs:\n",
    "1. `a_matrix`: The Bag-of-Words matrix where rows correspond to unique domains, and columns correspond to words.\n",
    "2. `domain_name`: The name of the domain for which the user wants to retrieve the word frequency.\n",
    "3. `word`: The specific word for which the user wants to check its frequency in the given domain.\n",
    "\n",
    "Here’s how the function works step-by-step:\n",
    "\n",
    "1. *Locate Domain Index*:  \n",
    "   `line_idx = unique_domains_list.index(domain_name)`  \n",
    "   This line finds the index of the domain name in the `unique_domains_list`, which corresponds to the row in the `bow_matrix`.\n",
    "\n",
    "2. *Locate Word Index*:  \n",
    "   `column_idx = word_list.index(word)`  \n",
    "   Similarly, this line retrieves the index of the word from `word_list`, representing the column in the BoW matrix.\n",
    "\n",
    "3. *Return Word Frequency*:  \n",
    "   `return(a_matrix[line_idx, column_idx])`  \n",
    "   The function then returns the value from the BoW matrix at the specified row (domain) and column (word), which represents how many times the word appears in the documents grouped under that domain.\n",
    "\n",
    "**Practical applications**\n",
    "\n",
    "This function is useful in various NLP and text mining tasks, especially for domain-specific text analysis:\n",
    "\n",
    "- *Biomedical research and discovery*: By retrieving the frequency of specific words across different domains, researchers can identify domain-specific terminology or emerging trends in literature.\n",
    "- *Word frequency analysis*: Users can easily extract how often a particular word is used in different domains, which can help in understanding language patterns or keyword presence across domains.\n",
    "\n",
    "**Use**\n",
    "\n",
    "To use this function:\n",
    "1. *Prepare Input*: Ensure you have the following inputs ready:\n",
    "   - `a_matrix`: The Bag-of-Words or TF-IDF matrix, where rows represent domains, and columns represent words.\n",
    "   - `unique_domains_list`: A list of unique domain names corresponding to the rows of the BoW matrix.\n",
    "   - `word_list`: A list of all words corresponding to the columns of the BoW matrix.\n",
    "   \n",
    "2. *Call the Function*:  \n",
    "   For example, to find how often the word `\"vcfs\"` appears in the domain `\"Autism\"`, call:\n",
    "   ```python\n",
    "   frequency = cell_value_in_a_matrix(bow_matrix, \"Autism\", \"vcfs\")\n",
    "   print(frequency)\n",
    "   ```\n",
    "\n",
    "The output will be the frequency of the word \"vcfs\" in the `\"Autism\"` domain, based on the aggregated data in the `bow_matrix`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_value_in_a_matrix(a_matrix: np.ndarray, domain_name: str, word: str) -> float:\n",
    "    \"\"\"\n",
    "    This function extracts the frequency of a specific word within a particular domain from an aggregated Bag-of-Words (BoW) matrix. \n",
    "    The `cell_value_in_a_matrix()` function takes three inputs:\n",
    "        a_matrix: the Bag-of-Words or TF-IDF matrix where rows correspond to unique domains, and columns correspond to words\n",
    "        domain_name: the name of the domain for which the user wants to retrieve the word frequency\n",
    "        word: the specific word for which the user wants to check its frequency in the given domain\n",
    "    \"\"\"\n",
    "    line_idx = unique_domains_list.index(domain_name)\n",
    "    if word in word_list:\n",
    "        column_idx = word_list.index(word)\n",
    "        return(a_matrix[line_idx, column_idx])\n",
    "    else:\n",
    "        return(0.0)\n",
    "\n",
    "print('Frequency of calmodulin in Autism: ', cell_value_in_a_matrix(domains_bow_matrix, 'Autism', 'calmodulin'))\n",
    "print('Frequency of calmodulin in Calcineurin: ', cell_value_in_a_matrix(domains_bow_matrix, 'Calcineurin', 'calmodulin'))\n",
    "\n",
    "print('Number of documents that contain calmodulin in Autism: ', cell_value_in_a_matrix(domains_binary_matrix, 'Autism', 'calmodulin'))\n",
    "print('Number of documents that contain calmodulin in Calcineurin: ', cell_value_in_a_matrix(domains_binary_matrix, 'Calcineurin', 'calmodulin'))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Frequency of vcfs in Autism: ', cell_value_in_a_matrix(domains_bow_matrix, 'Autism', 'vcfs'))\n",
    "print('Frequency of vcfs in Calcineurin: ', cell_value_in_a_matrix(domains_bow_matrix, 'Calcineurin', 'vcfs'))\n",
    "\n",
    "print('Number of documents that contain vcfs in Autism: ', cell_value_in_a_matrix(domains_binary_matrix, 'Autism', 'vcfs'))\n",
    "print('Number of documents that contain vcfs in Calcineurin: ', cell_value_in_a_matrix(domains_binary_matrix, 'Calcineurin', 'vcfs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the aggregated BoW matrix `domains_bow_matrix` and the aggregated `domains_binary_matrix`. The rows of these matrices correspond to the domains (*Autism*, *Calcineurin*) and the columns correspond to the words from the vocabulary.\n",
    "\n",
    "In the `domains_bow_matrix` the frequencies of the individual words are summed up, indicating how often the word occurs in the domain documents, while in the `domains_binary_matrix` the number of domain documents in which the word occurs is indicated (each word in a document counts as 1, even if it occurs many times in the document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define heuristics and ensemble framework\n",
    "\n",
    "def freqTerm(t: str) -> float:\n",
    "    return cell_value_in_a_matrix(domains_bow_matrix, unique_domains_list[0], t) + cell_value_in_a_matrix(domains_bow_matrix, unique_domains_list[1], t)\n",
    "\n",
    "print('1 freqTerm:', freqTerm('calmodulin'), freqTerm('vcfs'))\n",
    "print()\n",
    "\n",
    "def freqDoc(t: str) -> float:\n",
    "    return cell_value_in_a_matrix(domains_binary_matrix, unique_domains_list[0], t) + cell_value_in_a_matrix(domains_binary_matrix, unique_domains_list[1], t)\n",
    "\n",
    "print('2 freqDoc:', freqDoc('calmodulin'), freqDoc('vcfs'))\n",
    "print()\n",
    "\n",
    "def freqRatio(t: str) -> float:\n",
    "    if freqDoc(t) > 0:\n",
    "        return freqTerm(t) / freqDoc(t)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "print('3 freqRatio:', freqRatio('calmodulin'), freqRatio('vcfs'))\n",
    "print()\n",
    "\n",
    "def tfidfSum(t: str) -> float:\n",
    "    if t in word_list:\n",
    "        return sum_word_tfidf[t]\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "print('4 tfidfSum:', tfidfSum('calmodulin'), tfidfSum('vcfs'))\n",
    "print()\n",
    "\n",
    "def tfidfMax(t: str) -> float:\n",
    "    if t in word_list:\n",
    "        return max_word_tfidf[t]\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "print('5 tfidfMax:', tfidfMax('calmodulin'), tfidfMax('vcfs'))\n",
    "print()\n",
    "\n",
    "def out_freqTerm(t: str) -> float:\n",
    "    return cell_value_in_a_matrix(domains_outlier_bow_matrix, unique_domains_list[0], t) + cell_value_in_a_matrix(domains_outlier_bow_matrix, unique_domains_list[1], t)\n",
    "\n",
    "print('6 out_freqTerm:', out_freqTerm('calmodulin'), out_freqTerm('vcfs'))\n",
    "print()\n",
    "\n",
    "def out_freqDoc(t: str) -> float:\n",
    "    return cell_value_in_a_matrix(domains_outlier_binary_matrix, unique_domains_list[0], t) + cell_value_in_a_matrix(domains_outlier_binary_matrix, unique_domains_list[1], t)\n",
    "\n",
    "print('7 out_freqDoc:', out_freqDoc('calmodulin'), out_freqDoc('vcfs'))\n",
    "print()\n",
    "\n",
    "def out_freqRatio(t: str) -> float:\n",
    "    if out_freqDoc(t) > 0:\n",
    "        return out_freqTerm(t) / out_freqDoc(t)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "print('8 out_freqRatio:', out_freqRatio('calmodulin'), out_freqRatio('vcfs'))\n",
    "print()\n",
    "\n",
    "def out_tfidfSum(t: str) -> float:\n",
    "    return cell_value_in_a_matrix(domains_outlier_sum_tfidf_matrix, unique_domains_list[0], t) + cell_value_in_a_matrix(domains_outlier_sum_tfidf_matrix, unique_domains_list[1], t)\n",
    "\n",
    "print('9 out_tfidfSum:', out_tfidfSum('calmodulin'), out_tfidfSum('vcfs'))\n",
    "print()\n",
    "\n",
    "def out_tfidfMax(t: str) -> float:\n",
    "    return max(cell_value_in_a_matrix(domains_outlier_max_tfidf_matrix, unique_domains_list[0], t), cell_value_in_a_matrix(domains_outlier_max_tfidf_matrix, unique_domains_list[1], t))\n",
    "\n",
    "print('10 out_tfidfMax:', out_tfidfMax('calmodulin'), out_tfidfMax('vcfs'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined ten functions that compute heuristic scores of a given word from the vocabulary:\n",
    "\n",
    "1. freqTerm: \n",
    "2. freqDoc: \n",
    "3. freqRatio: \n",
    "4. tfidfSum: \n",
    "5. tfidfMax: \n",
    "6. out_freqTerm: \n",
    "7. out_freqDoc: \n",
    "8. out_freqRatio: \n",
    "9. out_tfidfSum: \n",
    "10. out_tfidfMax: \n",
    "\n",
    "These scores can be used as an estimations of the potential of b-term candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: domains_list, outlier_docs_list \n",
    "from collections import Counter\n",
    "\n",
    "# 1: count occurrences of domain names in the first list\n",
    "domains_counts = Counter(domains_list)\n",
    "\n",
    "# 2: filter the list using the selected indices\n",
    "filtered_list = [domains_list[i] for i in outlier_docs_list]\n",
    "\n",
    "# 3: count occurrences of domain names in the filtered list\n",
    "filtered_counts = Counter(filtered_list)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Count in original list: {domains_counts}\")\n",
    "print(f\"Count in filtered list: {filtered_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating heuristics, takes longer time for long vocabularies, for example 30 minutes for 80.000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Populate dictionaries for various heuristics\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Seed the random number generator for repeated randomness\n",
    "random.seed(321)\n",
    "\n",
    "print(\"Processing dictionaries of words ...\")\n",
    "\n",
    "selected_freqTerm = {}\n",
    "selected_freqDoc = {}\n",
    "selected_freqRatio = {}\n",
    "selected_tfidfSum = {}\n",
    "selected_tfidfMax = {}\n",
    "\n",
    "selected_out_freqTerm = {}\n",
    "selected_out_freqDoc = {}\n",
    "selected_out_freqRatio = {}\n",
    "selected_out_tfidfSum = {}\n",
    "selected_out_tfidfMax = {}\n",
    "\n",
    "selected_random = {}\n",
    "\n",
    "i = -1\n",
    "maxi = len(max_word_tfidf.keys())\n",
    "for word in max_word_tfidf.keys():\n",
    "    i += 1\n",
    "    if i % 10000 == 0:\n",
    "        print('Calculating heuristics score for the word index', i, 'of', maxi)\n",
    "    if sum_count_docs_containing_word[word] >= 1: # the word should appear in at least n documents, n = 1\n",
    "        passed = True\n",
    "        for domain_name in unique_domains_list:\n",
    "            if cell_value_in_a_matrix(domains_bow_matrix, domain_name, word) <= 0: # the word should appear at least once in each domain\n",
    "                passed = False\n",
    "        if passed:\n",
    "            selected_random[word] = random.randint(1, 999999)\n",
    "\n",
    "            selected_freqTerm[word] = freqTerm(word)\n",
    "            selected_freqDoc[word] = freqDoc(word)\n",
    "            selected_freqRatio[word] = freqRatio(word)\n",
    "            selected_tfidfSum[word] = tfidfSum(word)\n",
    "            selected_tfidfMax[word] = tfidfMax(word)\n",
    "\n",
    "            selected_out_freqTerm[word] = out_freqTerm(word)\n",
    "            selected_out_freqDoc[word] = out_freqDoc(word)\n",
    "            selected_out_freqRatio[word] = out_freqRatio(word)\n",
    "            selected_out_tfidfSum[word] = out_tfidfSum(word)\n",
    "            selected_out_tfidfMax[word] = out_tfidfMax(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a heuristic function for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_tfidf_selected = selected_tfidfSum\n",
    "\n",
    "import itertools\n",
    "print('All the words in the vocabulary: ', len(max_word_tfidf))\n",
    "print('Selected b-term candidate words: ', len(max_word_tfidf_selected), ' ', dict(itertools.islice(max_word_tfidf_selected.items(), 30)))\n",
    "\n",
    "max_word_tfidf_selected_sorted = LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf_selected, True)\n",
    "\n",
    "print('Sorted b-term candidate words: ', len(max_word_tfidf_selected_sorted), ' ', dict(itertools.islice(max_word_tfidf_selected_sorted.items(), 30)))\n",
    "print('The first and the last sorted b-term word: ', list(max_word_tfidf_selected_sorted.items())[0], ' ', list(max_word_tfidf_selected_sorted.items())[-1])\n",
    "print('Mean value of max TF-IDF values: ', np.array(list(max_word_tfidf_selected_sorted.values())).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the word dictionaries according to the heuristics scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_freqTerm_sorted = LBD_02_data_preprocessing.sort_dict_by_value(selected_freqTerm, True)\n",
    "selected_freqDoc_sorted = LBD_02_data_preprocessing.sort_dict_by_value(selected_freqDoc, True)\n",
    "selected_freqRatio_sorted = LBD_02_data_preprocessing.sort_dict_by_value(selected_freqRatio, True)\n",
    "selected_tfidfSum_sorted = LBD_02_data_preprocessing.sort_dict_by_value(selected_tfidfSum, True)\n",
    "selected_tfidfMax_sorted = LBD_02_data_preprocessing.sort_dict_by_value(selected_tfidfMax, True)\n",
    "\n",
    "selected_out_freqTerm_sorted = LBD_02_data_preprocessing.sort_dict_by_value(selected_out_freqTerm, True)\n",
    "selected_out_freqDoc_sorted = LBD_02_data_preprocessing.sort_dict_by_value(selected_out_freqDoc, True)\n",
    "selected_out_freqRatio_sorted = LBD_02_data_preprocessing.sort_dict_by_value(selected_out_freqRatio, True)\n",
    "selected_out_tfidfSum_sorted = LBD_02_data_preprocessing.sort_dict_by_value(selected_out_tfidfSum, True)\n",
    "selected_out_tfidfMax_sorted = LBD_02_data_preprocessing.sort_dict_by_value(selected_out_tfidfMax, True)\n",
    "\n",
    "selected_random_sorted = LBD_02_data_preprocessing.sort_dict_by_value(selected_random, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('freqTerm:', dict(itertools.islice(selected_freqTerm_sorted.items(), 7)))\n",
    "print('freqDoc:', dict(itertools.islice(selected_freqDoc_sorted.items(), 7)))\n",
    "print('freqRatio:', dict(itertools.islice(selected_freqRatio_sorted.items(), 7)))\n",
    "print('tfidfSum:', dict(itertools.islice(selected_tfidfSum_sorted.items(), 7)))\n",
    "print('tfidfMax:', dict(itertools.islice(selected_tfidfMax_sorted.items(), 7)))\n",
    "print()\n",
    "print('out_freqTerm:', dict(itertools.islice(selected_out_freqTerm_sorted.items(), 7)))\n",
    "print('out_freqDoc:', dict(itertools.islice(selected_out_freqDoc_sorted.items(), 7)))\n",
    "print('out_freqRatio:', dict(itertools.islice(selected_out_freqRatio_sorted.items(), 7)))\n",
    "print('out_tfidfSum:', dict(itertools.islice(selected_out_tfidfSum_sorted.items(), 7)))\n",
    "print('out_tfidfMax:', dict(itertools.islice(selected_out_tfidfMax_sorted.items(), 7)))\n",
    "print()\n",
    "print('random:', dict(itertools.islice(selected_random_sorted.items(), 7)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_freqTerm_sorted_bterms_list = list(selected_freqTerm_sorted.keys())\n",
    "selected_freqTerm_sorted_bterms_list_length = len(selected_freqTerm_sorted_bterms_list)\n",
    "\n",
    "df = pd.DataFrame({'b-term': selected_freqTerm_sorted_bterms_list, 'value': list(selected_freqTerm_sorted.values())})\n",
    "df[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (CONTEXT_SWITCH == 1) or (CONTEXT_SWITCH == 2):\n",
    "    name = 'bcl2'\n",
    "    if name in list(max_word_tfidf_selected_sorted.keys()):\n",
    "        print(name, ': ', 'position in the list of rare terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "              '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (CONTEXT_SWITCH == 1) or (CONTEXT_SWITCH == 2):\n",
    "    name = 'nitric'\n",
    "    print(name, ': ', 'position in the list of rare terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "          '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (CONTEXT_SWITCH == 1) or (CONTEXT_SWITCH == 2):\n",
    "    name = 'nitric oxide'\n",
    "    print(name, ': ', 'position in the list of rare terms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "          '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (CONTEXT_SWITCH == 1) or (CONTEXT_SWITCH == 2):\n",
    "    petric_bterms = [\"synaptic\", \"synaptic plasticity\", \"calmodulin\", \"radiation\", \"memory\", \n",
    "                     \"bcl2\", \"diabetes\", \"ulcerative colitis\", \"asbestos\", \"deletion syndrome\", \n",
    "                     \"22q112\", \"maternal hypothyroxinemia\", \"bombesin\"]\n",
    "else:\n",
    "    petric_bterms = []\n",
    "\n",
    "nn = 0\n",
    "indb = []\n",
    "size = len(max_word_tfidf_selected_sorted)\n",
    "for name in petric_bterms:\n",
    "    if name in max_word_tfidf_selected_sorted.keys():\n",
    "       nn += 1\n",
    "       position = list(max_word_tfidf_selected_sorted.keys()).index(name)\n",
    "       indb.append(position)\n",
    "       print(name, ': ', 'position in the list of potential bterms ', list(max_word_tfidf_selected_sorted.keys()).index(name), ' (', len(max_word_tfidf_selected_sorted), \\\n",
    "             '), max tfidf: ', format(max_word_tfidf_selected_sorted[name], '.3f'), ' part: ', format(position/size*100, '.1f'), sep='')\n",
    "    else:\n",
    "        print('NOT:', name, 'NOT in the list.')   \n",
    "print(nn, len(petric_bterms))\n",
    "mm = len(petric_bterms)\n",
    "print(indb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changed b-terms:\n",
    "\n",
    "- working memory -> memory\n",
    "- type 1 diabetes -> diabetes\n",
    "\n",
    "Missing b-terms:\n",
    "\n",
    "- 5hydroxytryptamine receptors - found only in Migraine !Mig documents\n",
    "- antimigraine - found only in Migraine !Mig documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the selected heuristics for bterm ranking ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Vocabulary size: {size}')\n",
    "print(f'No of selected b-terms: {len(indb)}')\n",
    "print()\n",
    "\n",
    "pt = []\n",
    "for i in range(size):\n",
    "    pt.append(0)\n",
    "for i in range(len(indb)):\n",
    "    for j in range(indb[i], size):\n",
    "        pt[j] += 1\n",
    "# print(pt)\n",
    "\n",
    "suma = 0\n",
    "part = 0\n",
    "for i in range(size):\n",
    "    # if i % 10000 == 0:\n",
    "        # print((i+1)/size*100.0, pt[i]/len(indb)*100.0)\n",
    "        # print((i+1)/size*100.0, pt[i]/mm*100.0)\n",
    "    part += pt[i]\n",
    "    suma += len(indb)\n",
    "# print(part/suma*100.0)\n",
    "\n",
    "no_all_bterm_candidates = size\n",
    "no_swansons_bterms = mm # len(indb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Display ROC curve and AUC (Area Under Curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example ROC curve points (y-values), replace with the actual list of 120 numbers\n",
    "roc_points = pt\n",
    "\n",
    "# X-values for the ROC curve (0 to 120)\n",
    "x_values = np.arange(0, no_all_bterm_candidates)\n",
    "\n",
    "# Calculating the AUC for the given ROC curve using the trapezoidal rule\n",
    "auc = np.trapz(roc_points, x_values) / (no_all_bterm_candidates*no_swansons_bterms) * 100  # Normalizing by the area of the full plot\n",
    "\n",
    "# Plotting the default curve (50% AUC)\n",
    "default_x = np.array([0, no_all_bterm_candidates])\n",
    "default_y = np.array([0, no_swansons_bterms])\n",
    "plt.plot(default_x, default_y, label='Default Curve (50% AUC)', linestyle='--', color='gray')\n",
    "\n",
    "# Plotting the given ROC curve\n",
    "plt.plot(x_values, roc_points, label=f'Given ROC Curve (AUC: {auc:.2f})', color='blue')\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.xlabel('False Positive Rate (Discrete)')\n",
    "plt.ylabel('True Positive Rate (Discrete)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
