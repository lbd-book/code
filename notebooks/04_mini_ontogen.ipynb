{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OntoGen workflow notebook tutorial based on Python script modules for LBD\n",
    "\n",
    "OntoGen [4] is a semi-automatic data-driven interactive text mining tool that assists users in the creative process of generating topic ontologies by grouping documents into related clusters. \n",
    "It is essentially a text mining tool for grouping documents into related clusters that can be viewed as concepts in an automatically created topic ontology. \n",
    "The underlying methodology is *ùëò*-means clustering, a particularly popular technique since only the *ùëò* parameter needs to be chosen to determine the number of categories into which documents should be clustered.\n",
    "\n",
    "<hr>\n",
    "\n",
    "[1] Petriƒç, I., Urbanƒçiƒç, T., Cestnik, B., Macedoni-Luk≈°iƒç, M. (2009). Literature mining method RaLoLink for uncovering relations between biomedical concepts. *Journal of Biomedical Informatics*, 42(2), pp. 219‚Äì227.\n",
    "\n",
    "[4] Fortuna, B., Grobelnik, M., Mladeniƒá, D. (2006). Semi-automatic data-driven ontology construction system, *Proceedings of the 9th International Multi-conference Information Society*, pp. 223‚Äì226.\n",
    "\n",
    "[5] Cestnik, B., Fabbretti, E., Gubiani, D., Urbanƒçiƒç, T., Lavraƒç, N. (2017). Reducing the search space in literature-based discovery by exploring outlier documents: a case study in finding links between gut microbiome and Alzheimer‚Äôs disease. *Genomics and computational biology*, 3(3), e58-1-e58-10. doi:10.18547/gcb.2017.vol3.iss3.e58.\n",
    "\n",
    "[6] Sluban, B., Jur≈°iƒç, M., Cestnik, B., Lavraƒç, N. (2012). Exploring the power of outliers for cross-domain literature mining. In M. R. Berthold (Ed.), Bisociative Knowledge Discovery (pp. 325‚Äì337). Springer.\n",
    "\n",
    "[7] Petriƒç, I., Cestnik, B., Lavraƒç, N., Urbanƒçiƒç, T. (2012). Outlier detection in crosscontext link discovery for creative literature mining. *The Computer Journal*, 55(1), 47‚Äì61.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Note that **our motive** was to **re-implement parts** of the tools such as **OntoGen**, RaJoLink and CrossBee so that we can generally **repeat the results** with the tools from the past experiments, and not so much to optimize the written Python code. The fucus was on understanding the learning processes and visualizing the workflows in terms of repeatability of the results obtained; the efficiency and elegance of the programming can be addressed in future versions of the scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and initialize `logging` library to track the execution of the scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Initialize logging with a basic configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s: %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LBD components from the framework notebooks. The description of the individual components from the framework notebooks can be found in the respective notebooks.\n",
    "\n",
    "The purpose of the **import_ipynb** library is to allow the direct import of Jupyter notebooks as modules so that code, functions and classes defined in one notebook can be easily reused in other notebooks or Python scripts.\n",
    "If the **import_ipynb** library is omitted (or commented out), the corresponding module will be imported from **.py** files exported from the **.ipynb** files. Note that importing from **.py** files is usually much faster and therefore more suitable for running scripts in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import import_ipynb\n",
    "import LBD_01_data_acquisition\n",
    "import LBD_02_data_preprocessing\n",
    "import LBD_03_feature_extraction\n",
    "import LBD_04_text_mining\n",
    "import LBD_05_results_analysis\n",
    "import LBD_06_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import additional Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the name of the domains $C$ and $A$, then load the responding text from the input file. The expected file format is as follows:\n",
    "\n",
    "1. The file is encoded in Ascii (if it is in UTF-8 or other encoding, it should be converted to Ascii).\n",
    "2. Each line in the file represents one document. The words in each document are separated by spaces. The length of the individual documents may vary.\n",
    "3. The first word in each line is the **unique id**, followed by a semicolon. Normally **pmid** (pubmed id) can be used for this purpose, alhough any unique id (e.g. **sequential count**) suffices.\n",
    "4. The second word in each line can optionally stand for a predefined domain (or class) of the document. In this case, the second word is preceded by **!**. For example, if the file contains documents that originate from two domains, e.g. *migraine* and *magnesium*, the second word in each line is either **!migraine** or **!magnesium**. If the file contains documents that originate from *autism* and *calcineurin*, the second word in each line will be either **!autism** or **!calcineurin**.\n",
    "5. If the second word is not preceded by **!**, it will be considered the first word of the document. In this case, the document will be given the domain **!NA** (**not applicable** or **not available**).\n",
    "\n",
    "\n",
    "**A background story for this experiment**\n",
    "\n",
    "First, we selected *autism* and *calcineurin* as our domains of interest.\n",
    "\n",
    "*Autism*  belongs to a group of pervasive developmental disorders that are portrayed by an\n",
    "early delay and abnormal development of cognitive, communication and social\n",
    "interaction skills of a person. It is a very complex and not yet sufficiently understood\n",
    "domain, where precise causes are still unknown; research suggests that it may be\n",
    "related to genetic mutations, environmental factors, and brain structure and function. \n",
    "*Calcineurin* is a protein phosphatase with a high prevalence in the brain.\n",
    "\n",
    "The dataset from the input file was constructed using the following PubMed query:\n",
    "\n",
    "1. autis* [TIAB] AND 1900/01/01:2007/12/31 [PDAT]\n",
    "2. calcineurin [TIAB] AND 1900/01/01:2007/12/31 [PDAT]\n",
    "\n",
    "The input file *input/f_autism_calcineurin.txt* was prepared for the experiments described in [1]. It contains 16.139 titles and abstracts, 10.819 from *autism* and 5.320 from *calcineurin*.\n",
    "\n",
    "In this experiment, we use **OntoGen** to detect outlier documents as described in in Chapter 6 *Outlier-based Closed Discovery* in section 6.3 *Outlier document detection and b-term identification through document clustering*.\n",
    "\n",
    "In the LBD outlier detection approach, each document from the two literatures is an instance represented by a set of words using frequency statistics based on the Bag Of Words (BoW) and Term Frequency‚ÄìInverse Document Frequency (TF-IDF) text representations.\n",
    "The BoW and TF-IDF vectors enable the content similarity of documents to be measured. Content similarity is calculated using OntoGen, where content similarity is measured by cosine distance and the standard TF-IDF word weighting measure, where a high frequency of co-occurring words in documents indicates high document similarity.\n",
    "The cosine similarity measure is used to position the documents according to their similarity to the representative document (centroid) of a selected domain.\n",
    "Documents positioned based on the cosine similarity measure can be visualized in OntoGen by a similarity graph with cosine similarity values falling in the interval [0, 1].\n",
    "The value 0 means extreme dissimilarity, i.e. two documents have no words in common, while the value 1 represents the similarity between two semantically identical documents in the BoW representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the global variable CONTEXT_SWITCH to the following values, depending on which domain pairs you would like to set as a context:\n",
    "# 1 for Autism-Calcineurin\n",
    "# 2 for Alzheimer-Macrobiota\n",
    "\n",
    "CONTEXT_SWITCH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CONTEXT_SWITCH == 1:\n",
    "    domainName = 'Autism-Calcineurin'\n",
    "    fileName = 'input/f_autism_calcineurin.txt'\n",
    "elif CONTEXT_SWITCH == 2:\n",
    "    domainName = 'Alzheimer-Macrobiota'\n",
    "    fileName = 'input/f_alzheimer_gimb.txt'\n",
    "\n",
    "lines = LBD_01_data_acquisition.load_data_from_file(fileName)\n",
    "# display the first 7 lines of the document\n",
    "[LBD_02_data_preprocessing.truncate_with_ellipsis(line, 110) for line in lines[:7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess the documents into a dictionary - might take a few minutes for longer files**\n",
    "\n",
    "The script in the next cell is used to prepare text data for further analysis in Literature-Based Discovery (LBD). The aim is to clean, standardize and structure the documents so that they are suitable for further tasks such as feature extraction, topic modeling and the discovery of hidden relationships in the literature. The script prepares the documents stored in `lines` in a dictionary.\n",
    "\n",
    "<details>\n",
    "  <summary>Click for more ...</summary>\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Creating a dictionary from raw data*: The script starts by converting a list of rows into a structured dictionary. \n",
    "    - *`construct_dict_from_list`*: this function takes the raw list of text lines (`lines`) and creates a dictionary (`docs_dict`) in which each entry typically represents a document, with a unique identifier as the key and the text of the document as the value.\n",
    "    - This conversion is important because it puts the text data into a more manageable format that allows efficient processing and retrieval.\n",
    "\n",
    "2. *Preprocessing of documents*: the script then applies various pre-processing steps to the documents:\n",
    "    - *Cleaning*: the text is cleaned to remove unwanted characters, punctuation and other errors.\n",
    "    - *Remove stop words*: frequent words that do not provide meaningful information (e.g. \"the\", \"and\") are removed.\n",
    "    - *Lemmatization*: words are reduced to their base or root form (e.g. \"running\" becomes \"run\") to ensure consistency.\n",
    "    - *Minimum word length*: words shorter than four characters are filtered out.\n",
    "    - *Keep only nouns*: the parameter `keep_only_nouns=True` ensures that only nouns (and proper nouns) are considered in further analysis, removing other word types like adjactives and verbs. \n",
    "    A trained pipeline from *spacy* named *en_core_web_md* is used for the task [https://spacy.io/models/en#en_core_web_md]. Note that this filter uses functions from an external *spacy* library to check every word in the vocabulary of the documents and is therefore time-consuming.\n",
    "    - *MeSH-specific filtering*: the parameter `keep_only_mesh=False` skips the MeSH filtering in this preprocessing.\n",
    "\n",
    "3. *Extract document IDs and processed text*:  the script then extracts lists of document IDs and the corresponding preprocessed text:\n",
    "    - *`extract_ids_list`*: returns a list of document IDs from the preprocessed dictionary to facilitate document lookup and management.\n",
    "    - *`extract_preprocessed_documents_list`*: extracts the cleaned and processed text for each document to prepare it for feature extraction or other analysis.\n",
    "By extracting these lists, the script organizes the data in a format that is easy to manipulate in subsequent steps, such as creating a Bag of Words (BoW) model or calculating TF-IDF scores.\n",
    "\n",
    "**Practical applications**\n",
    "\n",
    "- *Biomedical research and discovery*: This pre-processing approach is valuable in the biomedical field, where ensuring the relevance and accuracy of terms is critical to discovering new relationships between diseases, drugs and other biological concepts. By focusing on specific vocabularies such as MeSH, researchers can more effectively search the literature for new hypotheses or overlooked relationships.\n",
    "- *Data preparation for machine learning*: The cleaned and structured data generated by this script can be fed directly into machine learning models for tasks such as document classification or clustering.\n",
    "\n",
    "**Use**\n",
    "\n",
    "To use this script effectively:\n",
    "1. *Prepare the data*: Make sure you have a list of raw text lines (`lines`).\n",
    "2. *Execute the preprocessing steps*: Run the script to clean, filter and structure the text data.\n",
    "3. *Extract and analyze*: Use the extracted IDs and processed text for further analysis, e.g. to create models and visualizations or for exploratory research.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Creating a dictionary from raw data\n",
    "docs_dict = LBD_02_data_preprocessing.construct_dict_from_list(lines)\n",
    "\n",
    "# 2. Preprocessing of documents\n",
    "keep_list = []\n",
    "# Normally, the original domain names are removed from the vocabulary. If \"migraine\" and \"magnesium\" were added to the vocabulary during the comparison, \n",
    "# they might dominate the analysis because they are the focus of the study. The alorithms might attach extra importance to these terms \n",
    "# and push less obvious but potentially important terms or concepts into the background.\n",
    "if CONTEXT_SWITCH == 1:\n",
    "    remove_list = ['autism', 'calcineurin']\n",
    "elif CONTEXT_SWITCH == 2:\n",
    "    remove_list = ['alzheimer', 'gut', 'microbiota']\n",
    "else:\n",
    "    remove_list = []\n",
    "\n",
    "prep_docs_dict = LBD_02_data_preprocessing.preprocess_docs_dict(\n",
    "    docs_dict, keep_list = keep_list, remove_list = remove_list, mesh_word_list = [], \\\n",
    "    cleaning = True, remove_stopwords = True, lemmatization = True, \\\n",
    "    min_word_length = 4, keep_only_nouns = False, keep_only_mesh = False, stemming = False, stem_type = None)\n",
    "\n",
    "# 3. Extract document IDs and processed text\n",
    "ids_list = LBD_02_data_preprocessing.extract_ids_list(prep_docs_dict)\n",
    "prep_docs_list = LBD_02_data_preprocessing.extract_preprocessed_documents_list(prep_docs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three cells show the first dictionary entries, the document IDs (Pubmed) and the pre-processed documents.\n",
    "\n",
    "When displaying the first few dictionary entries, we can observe the difference between the original and the pre-processed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display the first 7 dictionary items\n",
    "truncated_dict = {\n",
    "    key: {sub_key: LBD_02_data_preprocessing.truncate_with_ellipsis(value, 110) for sub_key, value in sub_dict.items()}\n",
    "    for key, sub_dict in itertools.islice(prep_docs_dict.items(), 7)\n",
    "}\n",
    "truncated_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display the ids of the first 7 documents\n",
    "ids_list[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display the preprocessed text for the first 7 documents\n",
    "[LBD_02_data_preprocessing.truncate_with_ellipsis(line, 110) for line in prep_docs_list[:7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct BoW model from important words and n-grams**\n",
    "\n",
    "The next script continues the feature extraction process and focuses on refining a Bag of Words (BoW) model by filtering out less important terms and n-grams. It creates a Bag of Words matrix from the list of pre-processed documents. It then removes n-gram words that occur less than *min_ngram_count* times (in our case 3) in the entire document corpus. The words that are not contained in the MESH list *mesh_word_list* are also removed. This step is important to improve the quality and relevance of the text representation by reducing the vocabulary so that the following steps can be carried out more efficiently (in terms of time).\n",
    "\n",
    "<details>\n",
    "  <summary>Click for more ...</summary>\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Set parameters*: The script starts by setting the parameters for the n-gram size and the minimum document frequency:\n",
    "    - *`ngram_size`*: Specifies that the model considers pairs of consecutive words (bigrams) as features.\n",
    "    - *`min_df`*: Specifies the minimum number of documents in which a word or n-gram must occur in order to be included in the initial vocabulary.\n",
    "\n",
    "2. *Create Bag of Words representation*: The next step is to create the BoW model using the specified n-gram size.\n",
    "This function creates a vocabulary (`word_list`) from all terms and n-grams found in the preprocessed documents (`prep_docs_list`), together with the corresponding frequency matrix (`bow_matrix`). The output vocabulary includes all n-grams without filtering.\n",
    "\n",
    "3. *Filtering low-frequency n-grams*: The script then filters out n-grams that occur less frequently than a certain threshold:\n",
    "    - *`min_count_ngram`*: Specifies the minimum number of occurrences of n-grams to keep.\n",
    "    - The script calculates two important metrics:\n",
    "        - *document frequency*: How many documents contain each word or n-gram.\n",
    "        - *total frequency*: How often each word or n-gram appears in all documents.\n",
    "\n",
    "4. *Filtering based on specific criteria*: The script applies a more sophisticated filtering process to refine the vocabulary. The loop evaluates each term or n-gram in the vocabulary:\n",
    "   - *Non-n-grams*: Will only be retained if they are in a predefined `mesh_word_list`.\n",
    "   - *n-grams*: Are retained if:\n",
    "       - They fulfill the minimum frequency criteria.\n",
    "       - All partial words are contained in `mesh_word_list`.\n",
    "       - The n-gram does not consist of repeated words (e.g. \"word word\").\n",
    "\n",
    "\n",
    "5. *Applying the filters*: The script then filters both the rows and the columns of the BoW matrix. \n",
    "   - *`filter_matrix_columns`*: Refines the BoW matrix by retaining only the selected words or n-grams that meet the filter criteria.\n",
    "   - The updated vocabulary and matrix are then stored in `word_list` and `bow_matrix`, respectively.\n",
    "\n",
    "**Practical applications**\n",
    "\n",
    "- *Biomedical research and discovery*: This filtering method is particularly useful in medical research, where the focus is on extracting and analyzing relevant biomedical terms and concepts.\n",
    "- *Document analysis and classification*: By refining the feature set, this script can improve the performance of classifiers used in the categorization of scientific literature or other text corpora.\n",
    "- *Network analysis*: The filtered vocabulary can serve as a node in a network graph representing meaningful terms and their co-occurrence, which can be analyzed to detect hidden connections.\n",
    "\n",
    "**Use**\n",
    "\n",
    "To use this script effectively, you need to make sure you have a preprocessed document list (`prep_docs_list`). Adjust the parameters like `ngram_size`, `min_df` and `min_count_ngram` to your specific needs. After running the script, you will get a filtered vocabulary and a corresponding BoW matrix, which is more suitable for further analysis such as clustering, topic modeling or discovering new hypotheses in biomedical research.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Set parameters\n",
    "if CONTEXT_SWITCH == 1:\n",
    "    ngram_size = 1 # to reduce the vocabulary, only a single words are used for further analysis\n",
    "    min_df = 3\n",
    "elif CONTEXT_SWITCH == 2:\n",
    "    ngram_size = 1 \n",
    "    min_df = 1\n",
    "else:\n",
    "    ngram_size = 1 \n",
    "    min_df = 1\n",
    "\n",
    "# 2. Create Bag of Words representation\n",
    "word_list, bow_matrix = LBD_03_feature_extraction.create_bag_of_words(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary with all n-grams: ', len(word_list))\n",
    "\n",
    "# 3. Filtering low-frequency n-grams\n",
    "#    remove n-grams with frequency count less than min_count_ngram from vocabulary word_list and bow_matrix\n",
    "min_count_ngram = 3\n",
    "\n",
    "tmp_sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "tmp_sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "# 4. Filtering based on specific criteria\n",
    "tmp_filter_columns = []\n",
    "for i, word in enumerate(word_list):\n",
    "    if not LBD_03_feature_extraction.word_is_nterm(word):\n",
    "        tmp_filter_columns.append(i)\n",
    "    else:\n",
    "        if tmp_sum_count_word_in_docs[word] >= min_count_ngram:\n",
    "            tmp_filter_columns.append(i)\n",
    "\n",
    "# 5. Applying the filters\n",
    "#    keep the original order of rows\n",
    "tmp_filter_rows = []\n",
    "for i, id in enumerate(ids_list):\n",
    "    tmp_filter_rows.append(i)\n",
    "\n",
    "tmp_filtered_word_list, tmp_filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, bow_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "bow_matrix = tmp_filtered_bow_matrix\n",
    "print('Number of terms in preprocessed vocabulary after removing infrequent n-grams: ', len(word_list))\n",
    "\n",
    "# Output the lists for checking the order\n",
    "#LBD_02_data_preprocessing.save_list_to_file(word_list, \"output/_list.txt\")\n",
    "#LBD_02_data_preprocessing.save_list_to_file(prep_docs_list, \"output/_prep_list.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate relevant indicators for the BoW matrix**\n",
    "\n",
    "The script in the next cell is a continuation of the text preprocessing pipeline that calculates the margins for the Bag of Words (BoW) matrix and optimizes the BoW matrix for better interpretability and analysis. By arranging the matrix to highlight the most important terms and documents, this script helps to recognize patterns in the data, which is a crucial step in LBD.\n",
    "\n",
    "<details>\n",
    "  <summary>Click for more ...</summary>\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Counting word frequencies*: The script begins by calculating various frequency counts that provide insight into how words are distributed across documents:\n",
    "   - *`sum_count_docs_containing_word`*: Counts how many documents each word appears in.\n",
    "   - *`sum_count_word_in_docs`*: Totals the occurrences of each word across all documents.\n",
    "   - *`sum_count_words_in_doc`*: Tallies the total number of words in each document.\n",
    "\n",
    "   These metrics are essential for understanding the significance and distribution of terms within the corpus, which can guide further analysis.\n",
    "\n",
    "2. *Displaying frequency counts*: The script then prints a subset of these frequency counts to give an overview of the data:\n",
    "   - *`islice`* from `itertools` is used to print just the first few items, making it easier to inspect the data without overwhelming output.\n",
    "   - These print statements help users quickly assess the distribution and frequency of words and documents in the BoW model.\n",
    "\n",
    "3. *Optimizing the BoW matrix*: The script proceeds to rearrange the BoW matrix so that the most frequent words and documents are positioned at the top-left corner of the matrix:\n",
    "   - *sorting*: The words and documents are sorted by their frequencies in descending order.\n",
    "   - *filtering*: The indices of these sorted words and documents are then used to rearrange the BoW matrix.\n",
    "\n",
    "   This step ensures that the most significant terms and documents are easily accessible, facilitating further analysis such as clustering, topic modeling, or visualization.\n",
    "\n",
    "4. *Rearranging the matrix*: Finally, the script filters the matrix according to the computed order:\n",
    "   - *`filter_matrix`*: This function reorders the BoW matrix based on the sorted indices, ensuring that the most relevant terms and documents are emphasized.\n",
    "\n",
    "   The script then prints out the first few items in the reordered lists:\n",
    "   - This output allows users to verify that the matrix has been rearranged as intended, highlighting the most important elements of the dataset.\n",
    "\n",
    "**Use**\n",
    "\n",
    "To use this script, you must have a BoW matrix (`bow_matrix`) and the corresponding lists of words (`word_list`) and document IDs (`ids_list`). The script processes these inputs to calculate the frequency counts, reorder the matrix and output the reordered BoW matrix. This optimized matrix can be used for various downstream tasks, e.g. for creating visualizations, for deeper statistical analysis or as a basis for machine learning models for predictions.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Counting word frequencies\n",
    "sum_count_docs_containing_word = LBD_03_feature_extraction.sum_count_documents_containing_each_word(word_list, bow_matrix)\n",
    "\n",
    "sum_count_word_in_docs = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, bow_matrix)\n",
    "\n",
    "sum_count_words_in_doc = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, bow_matrix)\n",
    "\n",
    "# 2. Displaying frequency counts\n",
    "print('Number of documents in which each word is present: ', dict(itertools.islice(sum_count_docs_containing_word.items(), 7)))\n",
    "print('Number of occurences of each word in all documents: ', dict(itertools.islice(sum_count_word_in_docs.items(), 7)))\n",
    "print('Number of words in each document: ', dict(itertools.islice(sum_count_words_in_doc.items(), 7)))\n",
    "\n",
    "# 3. Optimizing the BoW matrix\n",
    "#    Compute the order of rows (documents) and columns (words) in the bow matrix so that the most frequent words are in the top-left corner. \n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_word_in_docs, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(sum_count_words_in_doc, reverse=True), ids_list) \n",
    "\n",
    "# 4. Rearranging the matrix\n",
    "#    Rearange (filter) the bow matrix according to the previously computed order.\n",
    "filtered_ids_list, filtered_word_list, filtered_bow_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, bow_matrix, filter_rows, filter_columns)\n",
    "\n",
    "print('The first few documents in the rows of the filtered bow matrix: ', filtered_ids_list[:7])\n",
    "print('The first few words in the columns of the filtered bow matrix: ', filtered_word_list[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize a part of BoW matrix**\n",
    "\n",
    "Visualize the upper left part of the Bag of Words (BoW) matrix. In the BoW matrix, each row corresponds to a document and each column to a word (or n-gram). The values in the matrix represent the frequency of the word in the corresponding document.\n",
    "As the BoW matrix mainly contains zeros, the displayed matrix is sorted so that the higher values in the cells are moved to the top left-hand corner of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_row = 0\n",
    "last_row = 20\n",
    "first_column = 0\n",
    "last_column = 15\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered Bag of Words', \\\n",
    "                                           filtered_bow_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], \\\n",
    "                                           filtered_word_list[first_column:last_column], as_int = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct TF-IDF matrix from important words and n-grams**\n",
    "\n",
    "The next script is designed to create a Term Frequency-Inverse Document Frequency (TF-IDF) matrix from a set of preprocessed documents and then refine this matrix by filtering out less relevant terms.\n",
    "\n",
    "<details>\n",
    "  <summary>Click for more ...</summary>\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Creating the TF-IDF matrix*:<br>\n",
    "   The script begins by generating a TF-IDF matrix using a list of preprocessed documents:\n",
    "   - *TF-IDF matrix*: This matrix represents the importance of each word (or n-gram) across all documents in the corpus.\n",
    "   - *`ngram_size`*: Specifies the size of word sequences to consider (e.g., unigrams, bigrams).\n",
    "   - *`min_df`*: Filters out terms that appear in fewer than a specified number of documents, reducing noise in the analysis.\n",
    "\n",
    "   This step is essential for transforming raw text data into a structured format that highlights important terms.\n",
    "\n",
    "2. *Rearranging the TF-IDF matrix*:\n",
    "   The script then refines the TF-IDF matrix by rearranging and filtering the terms:\n",
    "   - *filtering*: The matrix is filtered based on criteria such as the importance of terms, ensuring that only the most relevant words remain.\n",
    "   - *rearranging*: The matrix is reorganized according to a predefined order, based on the significance of terms or their relevance to specific documents.\n",
    "\n",
    "   This refinement process is crucial for improving the quality of the analysis by focusing on the most impactful terms, which can lead to more accurate and insightful results.\n",
    "\n",
    "**Use**\n",
    "\n",
    "Users can apply this script as part of a larger text mining workflow where the TF-IDF matrix serves as an important step in structuring and analyzing the data. By filtering and refining the matrix, users can ensure that their analysis focuses on the most relevant and meaningful terms, leading to more meaningful insights. In the context of LBD, this script is an essential tool for turning raw text data into actionable insights.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Creating the TF-IDF matrix\n",
    "word_list, tfidf_matrix = LBD_03_feature_extraction.create_tfidf(prep_docs_list, ngram_size, min_df)\n",
    "print('Number of terms in initial vocabulary with all n-grams: ', len(word_list))\n",
    "\n",
    "# 2. Rearranging the TF-IDF matrix\n",
    "#    Rearange (filter) the tfidf matrix according to the previously computed order from bow matrix.\n",
    "tmp_filtered_word_list, tmp_filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix_columns(\n",
    "    word_list, tfidf_matrix, tmp_filter_rows, tmp_filter_columns)\n",
    "\n",
    "word_list = tmp_filtered_word_list\n",
    "tfidf_matrix = tmp_filtered_tfidf_matrix\n",
    "print('Number of terms in preprocessed vocabulary after removing infrequent n-grams: ', len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute margins for TF-IDF matrix**\n",
    "\n",
    "This script is designed to analyze and manipulate Term Frequency-Inverse Document Frequency (TF-IDF) data for a corpus of documents. It computes various statistics related to the TF-IDF values for both words and documents, then filters the TF-IDF matrix to reorder it based on the most important words and documents. Note that the importance of words and documents is estimated from the calculated aggregates from TF-IDF matrix.\n",
    "\n",
    "<details>\n",
    "  <summary>Click for more ...</summary>\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Summing and maximizing TF-IDF values*:\n",
    "   - `sum_count_each_word_in_all_documents`: Calculates the sum of TF-IDF scores for each word across all documents, providing insight into the overall importance of words in the entire corpus.\n",
    "   - `max_tfidf_each_word_in_all_documents`: Finds the maximum TF-IDF score for each word, indicating the document where each word is most important.\n",
    "   - `sum_count_all_words_in_each_document`: Computes the sum of TF-IDF scores for all words in each document, which can be used to determine the \"weight\" or importance of the document itself.\n",
    "   - `max_tfidf_all_words_in_each_document`: Identifies the highest TF-IDF score for each document, which can help isolate which document contains particularly important terms.\n",
    "\n",
    "2. *Output statistics*:\n",
    "   - The script uses Python's `itertools.islice` function to print a preview of the top 7 values from each TF-IDF statistic. This offers a quick way to inspect the data without overwhelming the output with large lists.\n",
    "\n",
    "3. *Sorting and filtering the TF-IDF matrix*:\n",
    "   - After calculating the TF-IDF statistics, the script computes an ordering for the rows (documents) and columns (words) based on the maximum TF-IDF values. This ensures that the most important terms and documents are given priority in subsequent analyses.\n",
    "   - The `filter_matrix` function then reorders the original TF-IDF matrix based on these computed rankings, allowing for a focused view of the most significant content in the corpus.\n",
    "\n",
    "**Practical applications**\n",
    "\n",
    "- *Document analysis and classification*: By identifying the most important terms and documents in a corpus, this technique can assist in classifying documents into relevant categories.\n",
    "- *Term and key concept extraction*: Researchers can use the sum and max TF-IDF scores to isolate critical keywords that may represent novel concepts or ideas in the context of Literature-Based Discovery.\n",
    "- *Summarization and information retrieval*: By filtering out less important words and documents, this script can help narrow down a large corpus to the most relevant data, making retrieval tasks more efficient.\n",
    "\n",
    "**Use**\n",
    "\n",
    "This script is a practical tool for analyzing TF-IDF data in text mining applications. By summing and maximizing TF-IDF scores for words and documents, users can highlight the most significant elements of their corpus. The filtered matrix provides a more focused view of the most important terms, which is highly useful in fields like Literature-Based Discovery and NLP.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Summing and maximizing TF-IDF Values\n",
    "sum_word_tfidf = LBD_03_feature_extraction.sum_count_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "max_word_tfidf = LBD_03_feature_extraction.max_tfidf_each_word_in_all_documents(word_list, tfidf_matrix)\n",
    "\n",
    "sum_doc_tfidf = LBD_03_feature_extraction.sum_count_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "max_doc_tfidf = LBD_03_feature_extraction.max_tfidf_all_words_in_each_document(ids_list, tfidf_matrix)\n",
    "\n",
    "# 2. Output statistics\n",
    "print('Sum of TF-IDF for each word: ', dict(itertools.islice(sum_word_tfidf.items(), 7)))\n",
    "print('Max of TF-IDF for each word: ', dict(itertools.islice(max_word_tfidf.items(), 7)))\n",
    "\n",
    "print('Sum of TF-IDF for each document: ', dict(itertools.islice(sum_doc_tfidf.items(), 7)))\n",
    "print('Max of TF-IDF for each document: ', dict(itertools.islice(max_doc_tfidf.items(), 7)))\n",
    "\n",
    "# 3. Sorting and filtering the TF-IDF matrix\n",
    "#    Compute the order of rows (documents) and columns (words) in the TF-IDF matrix so that the most important words are in the top-left corner. \n",
    "filter_columns = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_word_tfidf, reverse=True), word_list)\n",
    "filter_rows = LBD_02_data_preprocessing.get_index_list_of_dict1_keys(\n",
    "    LBD_02_data_preprocessing.sort_dict_by_value(max_doc_tfidf, reverse=True), ids_list) \n",
    "\n",
    "#    Rearange (filter) the bow matrix according to the previously computed order.\n",
    "filtered_ids_list, filtered_word_list, filtered_tfidf_matrix = LBD_03_feature_extraction.filter_matrix(\n",
    "    ids_list, word_list, tfidf_matrix, filter_rows, filter_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize a part of TF-IDF matrix**\n",
    "\n",
    "Visualize the upper left part of the TF-IDF matrix. In the TF-IDF matrix, each row corresponds to a document and each column to a word (or n-gram). The values in the matrix represent the Term Frequency Inverse Document Frequency (abbreviated TF-IDF) of the word (term) in the corresponding document and document corpus. TF-IDF is a measure of how relevant a word in a document is in relation to a corpus: the measure increases proportionally to the number of occurrences of a word in the text, but is compensated for by the word frequency in the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_row, last_row, first_column, last_column = (0, 20, 0, 25)\n",
    "LBD_06_visualization.plot_bow_tfidf_matrix('Filtered TfIdf', filtered_tfidf_matrix[first_row:last_row,first_column:last_column], \\\n",
    "                                           filtered_ids_list[first_row:last_row], filtered_word_list[first_column:last_column], as_int = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of domain names of all documents (from the dictionary containing the documents) and a list of unique domain names. There are two distinct domains: *Autism* and *Calcineurin*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "domains_list = LBD_02_data_preprocessing.extract_domain_names_list(prep_docs_dict)\n",
    "print('Domain names for the first few documents: ', domains_list[:7])\n",
    "unique_domains_list = LBD_02_data_preprocessing.extract_unique_domain_names_list(prep_docs_dict)\n",
    "print('A list of all uniques domain names in all the documents: ', unique_domains_list)\n",
    "for unique_domain in unique_domains_list:\n",
    "    print('Number of documents in ', unique_domain, ': ', domains_list.count(unique_domain), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the documents of the two original domains in a 2D diagram by reducing the dimensionality of the TF-IDF matrix with PCA. You can experiment with the interactive diagram by clicking on the legend elements and showing or hiding the documents belonging to the selected element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LBD_06_visualization.visualize_tfidf_pca_interactive(ids_list, [], domains_list, tfidf_matrix, transpose = False, color_schema = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering of documents with k-means**\n",
    "\n",
    "This Python script performs the clustering of documents with the k-means algorithm from *sklearn*. It takes a TF-IDF matrix as input, groups documents into clusters based on their text similarity, and outputs the distribution of documents in each cluster. This script is a fundamental step for exploring and organizing large text corpora, providing deeper insights into text data and facilitating advanced tasks such as LBD.\n",
    "\n",
    "<details>\n",
    "  <summary>Click for more ...</summary>\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "The main function of the script is to categorize a collection of documents into a predefined number of clusters (`n_clusters`) based on their TF-IDF representations. This is how it works:\n",
    "\n",
    "1. *Clustering*: Uses the KMeans algorithm to divide documents into `n_clusters` groups based on their similarity in feature space.\n",
    "2. *Cluster assignments*: Assigns each document to a cluster and stores the labels indicating which cluster the document belongs to.\n",
    "3. *Summary*: Outputs a list of unique cluster labels and counts the number of documents in each cluster.\n",
    "\n",
    "**Practical applications**\n",
    "\n",
    "1. *Topic modeling*: Helps group documents by topic based on text similarity.\n",
    "2. *Document Categorization*: Organizes unstructured text data, such as research articles or web pages, into different categories.\n",
    "3. *Information Retrieval*: Improves search systems by clustering documents for better indexing and retrieval.\n",
    "4. *Literature Based Discovery (LBD)*: Identifies related research or conceptual clusters in large bibliographic datasets.\n",
    "\n",
    "**Use**\n",
    "\n",
    "1. *Prepare the input data*: First create a TF-IDF matrix of your documents using a library such as `TfidfVectorizer`.\n",
    "2. *Set parameters*: Set the number of clusters (`n_clusters`) based on your dataset and your targets.\n",
    "3. *Run the script*: Run the script to determine the cluster assignments and the number of documents per cluster.\n",
    "4. *Analyze the output*: Use the cluster information to examine the distribution of documents and recognize meaningful patterns or groupings in the data.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cluster the documents using k-means\n",
    "n_clusters = 2\n",
    "cluster_assignments = LBD_04_text_mining.perform_clustering(tfidf_matrix, n_clusters)\n",
    "\n",
    "unique_cluster_assignments = list(set(cluster_assignments))\n",
    "print('A list of all unique cluster names in all the documents: ', unique_cluster_assignments)\n",
    "for unique_cluster in unique_cluster_assignments:\n",
    "    print('Number of documents in ', unique_cluster, ': ', cluster_assignments.count(unique_cluster), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an interactive visualization of document clusters using k-means based on their TF-IDF matrix representations. The visualization reduces the high-dimensional TF-IDF space to two main components using PCA and colors each point (document) according to its cluster assignment. You can experiment with the interactive diagram by clicking on the legend items and showing or hiding the documents belonging to the selected item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LBD_06_visualization.visualize_tfidf_pca_interactive(ids_list, [], cluster_assignments, tfidf_matrix, transpose = False, color_schema = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of domain cluster combinations and extraction of relevant rows**\n",
    "\n",
    "This script assigns text domains to their respective clusters, extracts certain lines based on criteria and calculates the frequency of domain-cluster combinations. It uses the `Counter` class from the Python \"Collections\" module to determine the frequency of each combination. This script is a powerful tool for filtering and analyzing data in clustering workflows and enables a detailed examination of domain-cluster overlaps.\n",
    "\n",
    "<details>\n",
    "  <summary>Click for more ...</summary>\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "1. *Assign domains to clusters*:\n",
    "   - Combines each domain from `domains_list` with the corresponding cluster assignment (`cluster_assignments`), creating a `dl_ca` list of `domain-cluster` strings.\n",
    "   - Extracts specific lines based on the domain and cluster assignment conditions and appends them to the `outlier_lines` list.\n",
    "\n",
    "2. *Counting frequencies*:\n",
    "   - Uses `Counter` to calculate the frequency of each `domain-cluster` combination in `dl_ca`.\n",
    "   - Displays the number of each combination.\n",
    "\n",
    "3. *Output*:\n",
    "   - Outputs the count of all unique `domain-cluster` combinations and the `dl_ca` list.\n",
    "\n",
    "**Practical applications**\n",
    "\n",
    "1. *Cluster characterization*: Helps to identify the distribution of domains across different clusters.\n",
    "2. *Text filtering*: Enables extraction of specific text data based on clusters and domain relevance.\n",
    "3. *Data Analysis*: Provides a quick overview of the relationships between domains and clusters and helps to understand thematic patterns in text data.\n",
    "4. *LBD*: Supports targeted discovery by focusing on specific domain-cluster overlaps in the scientific literature.\n",
    "\n",
    "**Use**\n",
    "\n",
    "1. *Prepare input data*:\n",
    "   - Create `domains_list`, `cluster_assignments`, and `lines`, where:\n",
    "   - `domains_list` contains the domain names of the documents.\n",
    "   - `cluster_assignments` contains the cluster labels for the documents.\n",
    "   - `lines` contains the corresponding text data for each document.\n",
    "2. *Run the script*:\n",
    "   - The script creates `domain-cluster` combinations (`dl_ca`) and extracts lines that fulfill the defined conditions (depending of the domains used, e.g. `Autism` in cluster `1` or `Calcineurin` in cluster `0`; or `Alzheimer` in cluster `1` or `GIMB` in cluster `0`).\n",
    "3. *Analyze the output*:\n",
    "   - Check the frequency distribution of the domain-cluster combinations and the extracted lines (`outlier_lines`).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl_ca = []\n",
    "outlier_lines = []\n",
    "i = 0\n",
    "for dl, ca in zip(domains_list, cluster_assignments):\n",
    "    dl_ca.append(dl + '-' + ca)\n",
    "    if CONTEXT_SWITCH == 1:\n",
    "        # you have to decide, which combination of the original domain and the assigned cluster \n",
    "        # represents outlier documents\n",
    "        if (dl == 'Autism') and (ca == \"1\"):\n",
    "            outlier_lines.append(lines[i])        \n",
    "        if (dl == 'Calcineurin') and (ca == \"0\"):\n",
    "            outlier_lines.append(lines[i])\n",
    "    elif CONTEXT_SWITCH == 2:\n",
    "        if (dl == 'Alzheimer') and (ca == \"0\"):\n",
    "            outlier_lines.append(lines[i])        \n",
    "        if (dl == 'GIMB') and (ca == \"1\"):\n",
    "            outlier_lines.append(lines[i])\n",
    "    i += 1\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Use Counter to count the frequencies of each value\n",
    "frequencies = Counter(dl_ca)\n",
    "\n",
    "# Step 2: Display the frequencies\n",
    "for value, count in frequencies.items():\n",
    "    print(f\"Value {value} appears {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the interactive visualization of the documents from the four combinations of the original domains and the generated clusters with k-means. You can experiment with the interactive diagram by clicking on the legend elements and showing or hiding the documents belonging to the selected element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LBD_06_visualization.visualize_tfidf_pca_interactive(ids_list, [], dl_ca, tfidf_matrix, transpose = False, color_schema = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crosstab generator for frequency analysis**\n",
    "\n",
    "This script generates a crosstab matrix from two lists of values, showing the frequency of co-occurrence between items in the lists.\n",
    "\n",
    "<details>\n",
    "  <summary>Click for more ...</summary>\n",
    "\n",
    "**Functionality**\n",
    "\n",
    "The `create_crosstab` function is designed to take two equal-length lists as input and compute the frequency of their pairwise combinations. It uses Python's `defaultdict` to construct a two-dimensional frequency table. Here‚Äôs how it works:\n",
    "1. *Input Validation*: Ensures both lists have the same length to avoid data mismatches.\n",
    "2. *Data Processing*: Iterates over the paired elements of the lists, updating the frequency count in a nested dictionary.\n",
    "3. *Matrix Representation*: Organizes the results into a readable tabular format. It dynamically determines row and column headers based on the unique elements in the input lists and then displays the crosstab as a frequency table.\n",
    "\n",
    "The function `display_crosstab` returns the crosstab as a formatted string for display in a table format.\n",
    "\n",
    "**Example workflow**\n",
    "\n",
    "1. Replace `domains_list` and `cluster_assignments` in the function call with your lists of interest.\n",
    "2. Run the script to view a frequency matrix as a text-based table.\n",
    "3. Analyze the output to derive insights, such as identifying dominant relationships or clusters.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_crosstab(list1, list2):\n",
    "    # Check if the two lists have the same length\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"The two lists must have the same length.\")\n",
    "\n",
    "    # Initialize the crosstab dictionary\n",
    "    crosstab = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # Populate the crosstab with frequency counts\n",
    "    for val1, val2 in zip(list1, list2):\n",
    "        crosstab[val1][val2] += 1\n",
    "\n",
    "    return crosstab        \n",
    "\n",
    "def display_crosstab(crosstab):\n",
    "    headers = sorted(crosstab.keys()) # aaa\n",
    "    sub_headers = sorted({val for sublist in crosstab.values() for val in sublist.keys()}) # bbb\n",
    "\n",
    "    # Determine column widths for proper alignment\n",
    "    # col_width = max(len(str(x)) for x in sub_headers) + 3\n",
    "    col_width = max(len(str(x)) for x in headers + sub_headers + [v for row in crosstab.values() for v in row.values()]) + 2\n",
    "\n",
    "    # Create a header row\n",
    "    sub_header = [\" \" * col_width] + [f\"{bb:>{col_width}}\" for bb in sub_headers]\n",
    "    table = \" | \".join(sub_header) + \"\\n\" + \"-\" * len(\" | \".join(sub_header)) + \"\\n\"\n",
    "\n",
    "    # Add rows for each value in aaa\n",
    "    for header in headers:\n",
    "        row = [f\"{header:>{col_width}}\"]  # Row header\n",
    "        for sub_header in sub_headers:\n",
    "            value = crosstab.get(header, {}).get(sub_header, \"\")\n",
    "            row.append(f\"{value:>{col_width}}\")\n",
    "        table += \" | \".join(row) + \"\\n\"\n",
    "    \n",
    "    return table\n",
    "\n",
    "print(display_crosstab(create_crosstab(domains_list, cluster_assignments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sparse visualization of the documents randomly from only 10 percent of the documents. This visualization is useful to reduce the time to visualize all documents due to the large number of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selectors = []\n",
    "for i, dl in enumerate(domains_list):\n",
    "    if i % 10 == 0:\n",
    "        selectors.append(True)\n",
    "    else:\n",
    "        selectors.append(False)\n",
    "\n",
    "LBD_06_visualization.visualize_tfidf_pca_interactive([element for element, select in zip(ids_list, selectors) if select], [], [element for element, select in zip(dl_ca, selectors) if select], \n",
    "                                                     tfidf_matrix[selectors,:], transpose = False, color_schema = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have documents from two domains of interest, $A$ and $C$, we first train a clustering model that splits the documents into two clusters, $0$ and $1$, representing $A'$ and $C'$. \n",
    "It is assumed that there are overlaps between $0$ and $A'$ as well as $1$ and $B'$, or vice versa. \n",
    "\n",
    "The model created can then be used to classify all documents (i.e. the documents from $A \\cup C$). \n",
    "The documents that are misclassified according to their domain of origin (i.e. documents from $A$ that are classified as $C'$, and documents from $C$ that are classified as $A'$) are called **outlier documents**. \n",
    "These outliers are called borderline documents because, according to the model, they are more similar to the other domain than to the original domain.\n",
    "\n",
    "Now we display the number of outlier documents that are stored in a file for further processing. Note that the number of outlier documents is usually much smaller than the number of original documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Number of original documents:', len(lines))\n",
    "print('Number of outlier documents:', len(outlier_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the outlier documents to a file, adding '_outliers' to the end of original input file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the file path (or name) where the list outlier_lines will be saved\n",
    "\n",
    "# Split the full path into directory and file name\n",
    "tmp_path, tmp_file_name = os.path.split(fileName)\n",
    "# Remove the file extension from the file name\n",
    "tmp_file_name_without_ext = os.path.splitext(tmp_file_name)[0]\n",
    "# Add _outliers to the original filename\n",
    "file_path1 = tmp_path+'/'+tmp_file_name_without_ext+'_outliers.txt'\n",
    "\n",
    "# Open the file in write mode ('w') and write each string to the file\n",
    "with open(file_path1, 'w', encoding=\"ascii\") as file:\n",
    "    for line in outlier_lines:\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file with the outlier documents is prepared for further processing. \n",
    "\n",
    "Let us repeat that the main advantage of focusing on outlier documents is to improve the efficiency of the b-term search by reducing the search space of potential b-terms to those that occur in outlier documents. This significantly reduces the effort required to search for cross-domain bridging terms, as a much smaller subset of documents in which most $b$-terms occur must be examined. \n",
    "\n",
    "The experimental results in the standard migraine-magnesium domain as well as in the autism-calcineurin domain confirm the hypothesis that most bridging terms occur in outlier documents and that the search space for identifying b-terms can be greatly reduced by considering outlier documents [6, 7]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
