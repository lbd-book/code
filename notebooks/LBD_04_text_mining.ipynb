{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LBD Text Mining Module\n",
    "\n",
    "This module implements various text mining algorithms like topic modeling, sentiment analysis, and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from typing import List, Tuple, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function perform_topic_modeling takes a document-term matrix in the Compressed Sparse Row (CSR) format and an integer \n",
    "specifying the number of topics to extract as input, and uses the scikit-learn library to perform topic modeling using \n",
    "Latent Dirichlet Allocation (LDA). \n",
    "The function first creates an instance of the sklearn.decomposition.LatentDirichletAllocation class \n",
    "with the specified number of components and a fixed random state of 42 for reproducibility.\n",
    "\n",
    "The function then fits the LDA model to the input document-term matrix and transforms it into a document-topic matrix \n",
    "using the lda.fit_transform() method.\n",
    "\n",
    "Finally, the function returns a tuple containing the LDA model and the document-topic matrix as a NumPy array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_topic_modeling(matrix: csr_matrix, n_topics: int) -> Tuple[BaseEstimator, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform topic modeling on a document-term matrix using Latent Dirichlet Allocation (LDA).\n",
    "    :param matrix: csr_matrix, the document-term matrix to perform topic modeling on\n",
    "    :param n_topics: int, the number of topics to extract\n",
    "    :return: Tuple[BaseEstimator, np.ndarray], the fitted topic modeling LDA model and the document-topic matrix\n",
    "    \"\"\"\n",
    "    # Create an instance of the LatentDirichletAllocation class from scikit-learn\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "\n",
    "    # Fit the LDA model to the document-term matrix and transform it into a document-topic matrix\n",
    "    document_topic_matrix = lda.fit_transform(matrix)\n",
    "\n",
    "    # Return the LDA model and the document-topic matrix as a tuple\n",
    "    return lda, document_topic_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function perform_sentiment_analysis takes a list of tokenized text documents as input and uses \n",
    "the Natural Language Toolkit (NLTK) library to perform sentiment analysis using \n",
    "the VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tool. \n",
    "The function first ensures that the required NLTK data for the VADER sentiment analysis tool \n",
    "is downloaded using the nltk.download() method with the quiet=True argument to suppress download messages.\n",
    "\n",
    "The function then creates an instance of the nltk.sentiment.SentimentIntensityAnalyzer, \n",
    "which is an implementation of the VADER sentiment analysis algorithm. \n",
    "The function initializes an empty list sentiment_scores to store the sentiment scores for each document.\n",
    "\n",
    "For each document in the input tokens_list, the function calculates the sentiment score by first joining \n",
    "the tokens into a single string using the join() method. The function then calculates the sentiment score for the document \n",
    "using the SentimentIntensityAnalyzer's polarity_scores() method, which returns a dictionary containing \n",
    "the sentiment scores for the positive, negative, neutral, and compound sentiment components. \n",
    "The function appends the compound sentiment score to the sentiment_scores list.\n",
    "\n",
    "Finally, the function returns the list of sentiment scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_sentiment_analysis(tokens_list: List[List[str]]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis on a list of tokenized text documents using the VADER sentiment analysis tool from NLTK.\n",
    "    :param tokens_list: List[List[str]], a list of tokenized text documents\n",
    "    :return: List[float], a list of sentiment scores for each document\n",
    "    \"\"\"\n",
    "    # Ensure the NLTK data is downloaded\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "    \n",
    "    # Create an instance of the SentimentIntensityAnalyzer from NLTK\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Initialize an empty list to store the sentiment scores\n",
    "    sentiment_scores = []\n",
    "\n",
    "    # Calculate the sentiment scores for each document in the tokens_list\n",
    "    for tokens in tokens_list:\n",
    "        # Join the tokens into a single string\n",
    "        document = ' '.join(tokens)\n",
    "        \n",
    "        # Calculate the sentiment score using the SentimentIntensityAnalyzer\n",
    "        sentiment = sia.polarity_scores(document)\n",
    "        \n",
    "        # Append the compound sentiment score to the sentiment_scores list\n",
    "        sentiment_scores.append(sentiment['compound'])\n",
    "\n",
    "    return sentiment_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function perform_clustering takes a TF-IDF matrix and uses the scikit-learn library \n",
    "to perform clustering using the K-Means algorithm. The function first creates an instance \n",
    "of the sklearn.cluster.KMeans class with the specified number of clusters and a fixed random state of 321 for reproducibility.\n",
    "\n",
    "The function then fits the K-Means model to the input document-term matrix using the kmeans.fit() method. \n",
    "After fitting the model, the function obtains the cluster labels for each document using the kmeans.labels_ attribute.\n",
    "\n",
    "Finally, the function returns the list of cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_clustering(tfidf_matrix: np.ndarray, n_clusters: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Perform clustering on a document-term matrix using K-Means.\n",
    "    :param matrix: np.ndarray, the document-term matrix to perform clustering on\n",
    "    :param n_clusters: int, the number of clusters to create\n",
    "    :return: List[str], the KMeans fitted clustering model and the cluster labels for each document\n",
    "    \"\"\"\n",
    "    # Convert the input TF-IDF matrix into a numpy array to make it compatible with the KMeans algorithm\n",
    "    tfidf_array = np.asarray(tfidf_matrix)\n",
    "\n",
    "    # Create an instance of the KMeans class from scikit-learn and fit the KMeans model to the document-term matrix\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=321).fit(tfidf_array)\n",
    "\n",
    "    # Obtain the cluster labels for each document; labels are in string format\n",
    "    cluster_assignments = list(np.asarray(kmeans.labels_))\n",
    "    cluster_assignments = [str(i) for i in cluster_assignments]\n",
    "\n",
    "    # Return the cluster labels\n",
    "    return cluster_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function *find_document_outliers* prepares a list of *n_outliers* outlier documents based on TF-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_document_outliers(tfidf_matrix, n_outliers=1):\n",
    "    \"\"\"\n",
    "    Identify outliers based on cosine distance from the centroid of all document vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - tfidf_matrix: 2D numpy array of shape (n_docs, n_features), the TF-IDF matrix.\n",
    "    - n_outliers: number of documents to consider as outliers.\n",
    "    \n",
    "    Returns:\n",
    "    - outlier_indices: List of indices of the documents identified as outliers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Compute the centroid of the TF-IDF vectors\n",
    "    centroid = np.mean(tfidf_matrix, axis=0)\n",
    "    # print(centroid)\n",
    "    \n",
    "    # Step 2: Compute cosine distances between each document vector and the centroid\n",
    "    # Since we are computing cosine distance from the centroid, reshape centroid to (1, -1)\n",
    "    distances = cosine_distances(tfidf_matrix, centroid.reshape(1, -1)).flatten()\n",
    "    # print(distances)\n",
    "    \n",
    "    # Optional step 3: Determine the number of outliers to select (n_outliers is the parameter of the function)\n",
    "    # n_docs = tfidf_matrix.shape[0]\n",
    "    # n_outliers = int(np.ceil(outlier_fraction * n_docs))\n",
    "    \n",
    "    # Step 4: Find the indices of the top N documents with the highest distances\n",
    "    if n_outliers < 0:\n",
    "        outlier_indices = np.argsort(distances)[-n_outliers:]\n",
    "    else:\n",
    "        outlier_indices = np.argsort(distances)[-n_outliers:]\n",
    "    \n",
    "    return outlier_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the previous function, which finds a set of outlier documents, the nex function *find_word_outliers* finds *n_outliers* words from the *vocabulary*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_outliers(tfidf_matrix, vocabulary, n_outliers=1):\n",
    "    \"\"\"\n",
    "    Identify outlier words based on cosine distance from the centroid of all word vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - tfidf_matrix: 2D numpy array of shape (n_docs, n_features), the TF-IDF matrix.\n",
    "    - vocabulary: List of words corresponding to the columns (features) in the TF-IDF matrix.\n",
    "    - n_outliers: number of words to consider as outliers.\n",
    "    \n",
    "    Returns:\n",
    "    - outlier_words: List of words identified as outliers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Compute the centroid of the word vectors (columns)\n",
    "    centroid = np.mean(tfidf_matrix, axis=1)  # Centroid of columns (words)\n",
    "    # print(centroid)\n",
    "    \n",
    "    # Step 2: Compute cosine distances between each word vector and the centroid\n",
    "    # Since we are calculating cosine distance for words (columns), we transpose the matrix\n",
    "    distances = cosine_distances(tfidf_matrix.T, centroid.reshape(1, -1)).flatten()\n",
    "    # print(distances)\n",
    "    \n",
    "    # Optional step 3: Determine the number of outlier words to select (n_outliers is the parameter of the function)\n",
    "    # n_words = tfidf_matrix.shape[1]\n",
    "    # n_outliers = int(np.ceil(outlier_fraction * n_words))\n",
    "    \n",
    "    # Step 4: Find the indices of the top N words with the highest distances\n",
    "    if n_outliers < 0:\n",
    "        outlier_indices = np.argsort(distances)[:-n_outliers]\n",
    "    else:\n",
    "        outlier_indices = np.argsort(distances)[-n_outliers:]\n",
    "    \n",
    "    # Step 5: Retrieve the corresponding outlier words using the vocabulary\n",
    "    outlier_words = [vocabulary[i] for i in outlier_indices]\n",
    "    \n",
    "    return outlier_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
