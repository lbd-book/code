{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preprocessing module\n",
    "\n",
    "This module performs the preprocessing tasks for text mining such as tokenization, stop word removal, stemming and lemmatization.\n",
    "\n",
    "The principles and functions of text preprocessing are generally described in the accompanying book. They are also partly based on the following books, which are also recommended for further reading:\n",
    "\n",
    "<hr>\n",
    "\n",
    "[1] Kedia, A., Rasu, M. (2020): Hands-On Python Natural Language Processing: Explore tools and techniques to analyze and process text with a view to building real-world NLP applications, Pakt Publishing.\n",
    "\n",
    "[2] Antić, Z. (2021): Python Natural Language Processing Cookbook: Over 50 recipes to understand, analyze, and generate text for implementing language processing tasks, Pakt Publishing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bojan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bojan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp2 = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define string qualifiers that are used in the following functions\n",
    "\n",
    "strDomainDefault = 'NA' # 'default'\n",
    "strIdDefault = 'tmp'\n",
    "strDomainKey = 'domain'\n",
    "strDocumentKey = 'document'\n",
    "strPreprocessedKey = 'preprocessed'\n",
    "strPreprocessedDefaultValue = ['NA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for displaying longer strings truncated to a specified length with added ellipsis (...) if it's longer than the specified length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_with_ellipsis(string: str, length: int) -> str:\n",
    "    \"\"\"\n",
    "    Truncate a string to a specified length and add ellipsis (...) if it's longer than the specified length.\n",
    "    \n",
    "    Parameters:\n",
    "    - string (str): the string to be truncated\n",
    "    - length (int): the maximum allowed length of the truncated string\n",
    "    \n",
    "    Returns:\n",
    "    - str: the truncated string with ellipsis (...) if truncation occurred\n",
    "    \"\"\"\n",
    "    if len(string) > length:\n",
    "        return string[:length - 3] + '...'  # Adjust for the length of the ellipsis\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fuction *construct_dict_from_list* is designed to process a list of documents *docs_list* and transform them into a structured dictionary format. Each document in the list is expected to potentially have an identifier *id:* and a domain *!domain* at the beginning, followed by the main content of the document. The procedure aims to extract these components and store them in a structured manner within a dictionary. The structure of the returned dictionary is the following:\n",
    "<code>\n",
    "{pubmed_id: {domain: 'str', document: 'str', preprocessed: 'str'}, ...\n",
    "}</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dict_from_list(docs_list: List[str]) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Transform a list of documents (strings) to dictionary, such that \n",
    "    the first word in the document (if terminated by :) becomes a key label in the dictionary\n",
    "    and the second word in the document (if preceeded by !) becomes a domain label in the subdictionary\n",
    "    end the rest of the document becomes a document string in the subdictionary\n",
    "    :param docs_list: List[str], a list of documents\n",
    "    :return: Dict[str, Dict], a dictionary where the first word is the key to subdictionary with domain name and document text; \n",
    "             in subdictionary there is also a placeholder for preprocessed text\n",
    "    \"\"\"\n",
    "    processed_dict = {}\n",
    "    tmp_id = -1\n",
    "    for doc in docs_list:\n",
    "        tmp_id += 1\n",
    "        # Extract the original id: and !domain from the document doc, if they exist\n",
    "        tokens = doc.split()\n",
    "        tmp_domain = strDomainDefault\n",
    "\n",
    "        doc_id = tokens[0]\n",
    "        if doc_id[-1] == ':':\n",
    "            pubmed_id = doc_id[:-1]\n",
    "            tokens.pop(0)\n",
    "        else:\n",
    "            pubmed_id = strIdDefault + str(tmp_id)\n",
    "\n",
    "        doc_domain = tokens[0]\n",
    "        if doc_domain[0] == '!':\n",
    "            pubmed_domain = doc_domain[1:]\n",
    "            tokens.pop(0)\n",
    "        else:\n",
    "            pubmed_domain = tmp_domain\n",
    "\n",
    "        # Store the domain name and the document in a new dictionary\n",
    "        processed_dict[pubmed_id] = {\n",
    "            strDomainKey: pubmed_domain,\n",
    "            strDocumentKey: ' '.join(tokens),\n",
    "            strPreprocessedKey: strPreprocessedDefaultValue\n",
    "        }\n",
    "    return processed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *construct_dict_from_lists* constructs a document dictionary from lists of ids, domains and documents (original and preprocessed). All the input lists are of the same length; the order of the items in the input lists are ordered correspondingly to the list of documents *docs_list*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a document dictionary from lists of ids, domains and documents (original and preprocessed)\n",
    "def construct_dict_from_lists(ids_list: List[str], domains_list: List[str], docs_list: List[str], prep_docs_list: List[str]) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Construct a dictionary from three lists: ids, domains and docs, where all the lists are of same length and\n",
    "    the list items with the same index correspond to the same document\n",
    "    :param ids_list: List[str], a list of ids (e.g. pubmed_ids)\n",
    "    :param domains_list: List[str], a list of domains\n",
    "    :param docs_list: List[str], a list of documents\n",
    "    :return: Dict[str, Dict], a dictionary where the first word is the key to subdictionary with domain name and document text\n",
    "    \"\"\"\n",
    "    processed_dict = {}\n",
    "    for num in range(len(ids_list)):\n",
    "        # Combine and store the list elements to the dictionary\n",
    "        processed_dict[ids_list[num]] = {\n",
    "            strDomainKey: domains_list[num],\n",
    "            strDocumentKey: docs_list[num],\n",
    "            strPreprocessedKey: prep_docs_list[num]\n",
    "        }\n",
    "    return processed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The foolowing functions are used to extract list (*ids_list*, *domain_names_list*, *documents_list* and *preprocessed_documents_list*) from the dictionary *docs_dict*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ids_list(docs_dict: Dict[str, Dict]) -> List[str]:\n",
    "    processed_list = []\n",
    "    for pubmed_id, doc in docs_dict.items():\n",
    "        # Extract the id (key) from the document dictionary\n",
    "        processed_list.append(pubmed_id)\n",
    "    return processed_list\n",
    "\n",
    "def extract_domain_names_list(docs_dict: Dict[str, Dict]) -> List[str]:\n",
    "    processed_list = []\n",
    "    for pubmed_id, doc in docs_dict.items():\n",
    "        # Extract the original domain name from the document dictionary\n",
    "        processed_list.append(doc[strDomainKey])\n",
    "    return processed_list\n",
    "\n",
    "def extract_unique_domain_names_list(docs_dict: Dict[str, Dict]) -> List[str]:\n",
    "    processed_list = []\n",
    "    for pubmed_id, doc in docs_dict.items():\n",
    "        # Extract the original domain name from the document dictionary and store in not already present\n",
    "        if doc[strDomainKey] not in processed_list:\n",
    "            processed_list.append(doc[strDomainKey])\n",
    "    return processed_list\n",
    "\n",
    "def extract_documents_list(docs_dict: Dict[str, Dict]) -> List[str]:\n",
    "    processed_list = []\n",
    "    for pubmed_id, doc in docs_dict.items():\n",
    "        # Extract the original document string from the the document dictionary\n",
    "        processed_list.append(doc[strDocumentKey])\n",
    "    return processed_list\n",
    "\n",
    "def extract_preprocessed_documents_list(docs_dict: Dict[str, Dict]) -> List[str]:\n",
    "    processed_list = []\n",
    "    for pubmed_id, doc in docs_dict.items():\n",
    "        # Extract the original document string from the the document dictionary\n",
    "        processed_list.append(doc[strPreprocessedKey])\n",
    "    return processed_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for saving lists and dictionaries to text files for further documentation and inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_file(my_list: List, filename: str):\n",
    "    \"\"\"\n",
    "    Save the provided list to a text file.\n",
    "    \n",
    "    Parameters:\n",
    "    - my_list (list): The list of elements to be saved.\n",
    "    - filename (str): The name of the file where the list should be saved.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        for item in my_list:\n",
    "            file.write(\"%s\\n\" % item)\n",
    "\n",
    "def sort_dict_by_value(d: Dict, reverse = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Sort a dictionary by the value in ascending (default) or descending (reverse = True) order.\n",
    "    The values in the dictionary should be elementary (int, float, str)\n",
    "    \n",
    "    Parameters:\n",
    "    - d (dict): The dictionary of elements to be sorted.\n",
    "    - reverse (boolean): The reverse order of sorting.\n",
    "    \"\"\"\n",
    "    return dict(sorted(d.items(), key = lambda x: x[1], reverse = reverse))\n",
    "\n",
    "def get_index_list_of_dict1_keys(dict1: Dict, list2: List):\n",
    "    \"\"\"\n",
    "    Return a list of indeces for dict1 keys in the second dictionary dict2.\n",
    "    \n",
    "    Parameters:\n",
    "    - dict1, dict2 (dict): The two dictionaries.\n",
    "    \"\"\"\n",
    "    ind_list = []\n",
    "    for key, value in dict1.items():\n",
    "        ind_list.append(list2.index(key))\n",
    "    return ind_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function cleans text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_clean_text(corpus, keep_list, remove_list):\n",
    "    \"\"\"\n",
    "    Purpose: function to keep only alphabets; short words (single character), digits and certain words (punctuations, tabs etc.) removed\n",
    "    \n",
    "    Input: a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
    "           even after the cleaning process, and words in 'remove_list', which have to be removed unconditionally \n",
    "    \n",
    "    Output: the cleaned text corpus\n",
    "    \n",
    "    \"\"\"\n",
    "    cleaned_corpus = []\n",
    "    for row in corpus:\n",
    "        qs = []\n",
    "        for word in row.split():\n",
    "            word = word.lower()\n",
    "            # the pattern in re.sub determines which characters are accepted as valid \n",
    "            pattern = '[^a-z0-9čšž]'\n",
    "            p1 = re.sub(pattern, '', word)\n",
    "            if p1 not in keep_list:\n",
    "                if p1 not in remove_list:\n",
    "                    if (not p1.isdigit()) and (len(p1) >= 2):\n",
    "                        qs.append(p1)\n",
    "            else:\n",
    "                qs.append(p1)\n",
    "        cleaned_corpus.append(' '.join(qs))\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is partially based on [1]\n",
    "def do_remove_stopwords(corpus):\n",
    "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for word in wh_words:\n",
    "        stop.remove(word)\n",
    "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is partially based on [1]\n",
    "def do_lemmatize(corpus):\n",
    "    lem = WordNetLemmatizer()\n",
    "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is partially based on [1]\n",
    "def do_stem(corpus, stem_type = None):\n",
    "    if stem_type == 'snowball':\n",
    "        stemmer = SnowballStemmer(language = 'english')\n",
    "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
    "    else :\n",
    "        stemmer = PorterStemmer()\n",
    "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_keep_only_longer_or_equal_length_words(corpus, min_length):\n",
    "    modified_corpus = []\n",
    "    for sublist in corpus:\n",
    "        modified_sublist = []\n",
    "        for term in sublist:\n",
    "            # if (len(term) >= min_length) and (not term[0].isdigit()):\n",
    "            if (len(term) >= min_length):\n",
    "                modified_sublist.append(term)\n",
    "        modified_corpus.append(modified_sublist)\n",
    "    return modified_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_remove_non_nouns(corpus, keep_list):\n",
    "    modified_corpus = []\n",
    "    for sublist in corpus:\n",
    "        modified_sublist = []\n",
    "        doc = nlp2(' '.join(sublist))\n",
    "        pos_terms = [(token.text, token.pos_) for token in doc]\n",
    "        for term in pos_terms:\n",
    "            # if (term[1] == 'NOUN') or (term[1] == 'PROPN') or (term[1] == 'ADJ') or (term[1] == 'VERB'):\n",
    "            if term[0] in keep_list:\n",
    "                modified_sublist.append(term[0])\n",
    "            elif (term[1] == 'NOUN') or (term[1] == 'PROPN'):\n",
    "                modified_sublist.append(term[0])\n",
    "        modified_corpus.append(modified_sublist)\n",
    "    return modified_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_remove_non_mesh(corpus, mesh_word_list):\n",
    "    modified_corpus = []\n",
    "    for sublist in corpus:\n",
    "        modified_sublist = []\n",
    "        for term in sublist:\n",
    "            if term in mesh_word_list:\n",
    "                modified_sublist.append(term)\n",
    "        modified_corpus.append(modified_sublist)\n",
    "    return modified_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is partially based on [1]\n",
    "def preprocess(corpus, keep_list, remove_list, mesh_word_list, \\\n",
    "               cleaning = True, remove_stopwords = True, lemmatization = True, \\\n",
    "               min_word_length = 0, keep_only_nouns = False, keep_only_mesh = False, stemming = False, stem_type = None):\n",
    "    \"\"\"\n",
    "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
    "    \n",
    "    Input : \n",
    "    'corpus' - Text corpus on which pre-processing tasks will be performed - in the form of a list of strings - documents\n",
    "    'keep_list' - List of words to be retained during cleaning process\n",
    "    'min_word_length' - minimal length of word to be kept after the preprocessing; default 0 means all the wors are kept\n",
    "    'cleaning', 'remove_stopwords', 'lemmatization', 'stemming', 'keep_only_nouns' - Boolean variables indicating whether \n",
    "                                                                  a particular task should be performed or not\n",
    "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
    "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
    "    \n",
    "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
    "    \n",
    "    Output : Returns the preprocessed text corpus - in the form of a list of documents (each document is a string of words)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if cleaning == True:\n",
    "        logging.info('Text cleaning ...')\n",
    "        corpus = do_clean_text(corpus, keep_list, remove_list)\n",
    "    \n",
    "    if remove_stopwords == True:\n",
    "        logging.info('Removing stopwords ...')\n",
    "        corpus = do_remove_stopwords(corpus)\n",
    "    else:\n",
    "        corpus = [[x for x in x.split()] for x in corpus]\n",
    "    \n",
    "    if lemmatization == True:\n",
    "        logging.info('Lemmatization ...')\n",
    "        corpus = do_lemmatize(corpus)\n",
    "\n",
    "    if min_word_length > 0:\n",
    "        logging.info('Keeping only longer words (>= ' + str(min_word_length) + ' characters)...')\n",
    "        corpus = do_keep_only_longer_or_equal_length_words(corpus, min_word_length) # add keep_list\n",
    "\n",
    "    if keep_only_nouns:\n",
    "        logging.info('Keeping only nouns ...')\n",
    "        corpus = do_remove_non_nouns(corpus, keep_list) # add keep_list\n",
    "\n",
    "    if keep_only_mesh:\n",
    "        logging.info('Keeping only selected MeSH terms ...')\n",
    "        corpus = do_remove_non_mesh(corpus, mesh_word_list) # add keep_list\n",
    "\n",
    "    if stemming == True:\n",
    "        logging.info('Stemming ...')\n",
    "        corpus = do_stem(corpus, stem_type)\n",
    "\n",
    "    corpus = [' '.join(x) for x in corpus]  \n",
    "    logging.info('Preprocessing finished.')      \n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *preprocess_docs_dict* function performs various pre-processing tasks on a dictionary of documents. The function first extracts the IDs, domain names, and actual documents from the docs_dict. It then preprocesses the text by potentially cleaning, stemming, lemmatizing, and removing stopwords based on the provided boolean flags. The processed corpus is then combined back with the IDs and domain names to form a new dictionary that is returned by the function.\n",
    "\n",
    "Note: One should either use stemming or lemmatization, but not both, as there is no added benefit to using them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_docs_dict(docs_dict, keep_list, remove_list, mesh_word_list, \\\n",
    "               cleaning = True, remove_stopwords = True, lemmatization = True, \\\n",
    "               min_word_length = 0, keep_only_nouns = False, keep_only_mesh = False, stemming = False, stem_type = None):\n",
    "    \"\"\"\n",
    "    Purpose : Perform the pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal) on the docs_dict\n",
    "    \n",
    "    Input : \n",
    "    'docs_dict' - Dictionary of documents on which the preprocessing tasks are performed\n",
    "    'keep_list' - List of words to be retained during cleaning process\n",
    "    'remove_list' - List of words to be removed during cleaning process\n",
    "    'mesh_word_list' - List of words to be kept during cleaning process, depending on keep_only_mesh\n",
    "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should \n",
    "                                                                  be performed or not\n",
    "    'min_word_length', 'keep_only_nouns', 'keep_only_mesh' - Integer and Boolean variables determining the preprocessing steps\n",
    "    'stem_type' - Choose between Porter stemmer or Snowball (Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
    "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
    "    \n",
    "    Output : Returns the processed corpus in the form of a dictionary with the structure descibed earlier.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ids_list = extract_ids_list(docs_dict)\n",
    "    domains_list = extract_domain_names_list(docs_dict)\n",
    "    corpus = extract_documents_list(docs_dict)\n",
    "\n",
    "    corpus_with_preprocessing = preprocess(corpus, keep_list = keep_list, remove_list = remove_list, mesh_word_list = mesh_word_list, \\\n",
    "                cleaning = cleaning, remove_stopwords = remove_stopwords, lemmatization = lemmatization, \\\n",
    "                min_word_length = min_word_length, keep_only_nouns = keep_only_nouns, keep_only_mesh = keep_only_mesh, \\\n",
    "                stemming = stemming, stem_type = stem_type)\n",
    "\n",
    "    return construct_dict_from_lists(ids_list, domains_list, corpus, corpus_with_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
