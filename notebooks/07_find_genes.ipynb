{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a879503-7c5e-46e6-abe2-026ca36f64f6",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793e9ff-e2ac-44f2-844d-41bf4ec2c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install packages. WARNING: if you run this on windows platform install 'pytorch' \n",
    "#!pip install nltk==3.7 pandas scipy==1.10.1 fasttext==0.9.2 gensim==4.3.2 scikit-learn==1.5.0 torch --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ecd23-b63e-456b-8b46-bc3507eaf153",
   "metadata": {},
   "source": [
    "We need a Facebook MUSE library to embedding alignment. You can only get this by cloning the Github repository into the project.\n",
    "Go to the project folder and run the following command in the console:\n",
    "\n",
    "    git clone https://github.com/facebookresearch/MUSE\n",
    "\n",
    "After you have cloned it, go to the MUSE/src/utils.py file and change 'fastText' to 'fasttext' in lines 76 and 80 (i.e. change the capital T to t), otherwise the code will crash. Run the Muse script for alignment of embeddings from the previous step (CAUTION: Replace slashes with backslashes in the path specification if you have a Windows platform):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f3c304-c01c-4fb5-bf49-f58f8bc83d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Facebook MUSE repository into the current directory:\n",
    "#!git clone https://github.com/facebookresearch/MUSE\n",
    "\n",
    "#!sed -i 's/import fastText/import fasttext/' MUSE/src/utils.py\n",
    "#!sed -i 's/return fastText\\.load_model/return fasttext.load_model/' MUSE/src/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb8f3d-de84-45b9-8124-5a3d74dc7e8d",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149fe840-3a49-40dc-8529-0e301b3c72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fasttext\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ffad0-2f7d-47a3-83e0-baca8afe88db",
   "metadata": {},
   "source": [
    "# Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225794d0-6d79-4f20-9b90-169109623f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create datasets for embedding training and dictionaries for embedding alignement\n",
    "def get_datasets():\n",
    "\n",
    "    pd_file = open('input/pd.txt', 'w', encoding=\"utf8\")\n",
    "    cr_file = open('input/cr.txt', 'w', encoding=\"utf8\")\n",
    "\n",
    "    with open('input/C_merged_PD_in_CR_trunc_clean.txt', 'r', encoding='utf8') as f:\n",
    "        vocab = {}\n",
    "        for line in f:\n",
    "            if len(line.strip()) > 0:\n",
    "                id = line.split()[0]\n",
    "                c = line.split()[1]\n",
    "                text = \" \".join(line.split()[2:]).strip()\n",
    "                text = \" \".join(nltk.wordpunct_tokenize(text)).lower()\n",
    "\n",
    "                if c == \"!CR\":\n",
    "                    cr_file.write(text + '\\n')\n",
    "                    words = text.split()\n",
    "                    for w in words:\n",
    "                        if w in vocab:\n",
    "                            vocab[w][0] += 1\n",
    "                        else:\n",
    "                            vocab[w] = [1, 0]\n",
    "                elif c == \"!PD\":\n",
    "                    pd_file.write(text + '\\n')\n",
    "                    words = text.split()\n",
    "                    for w in words:\n",
    "                        if w in vocab:\n",
    "                            vocab[w][1] += 1\n",
    "                        else:\n",
    "                            vocab[w] = [0, 1]\n",
    "        pd_file.close()\n",
    "        cr_file.close()\n",
    "    words = []\n",
    "    for word, freq in vocab.items():\n",
    "        if freq[0] > 0 and freq[1] > 2:\n",
    "            words.append((word, freq[0], freq[1]))\n",
    "\n",
    "    words = sorted(words, reverse=True, key= lambda x: x[-1])\n",
    "    train = open('input/en_en_dict_train.txt', 'w', encoding='utf8')\n",
    "    test = open('input/en_en_dict_test.txt', 'w', encoding='utf8')\n",
    "\n",
    "    counter = 0\n",
    "    for w, f1, f2, in words[:5000]:\n",
    "        if counter % 3 == 0:\n",
    "            test.write(w + '\\t' + w + '\\n')\n",
    "        else:\n",
    "            train.write(w + '\\t' + w + '\\n')\n",
    "        counter += 1\n",
    "    train.close()\n",
    "    test.close()\n",
    "\n",
    "\n",
    "get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26889591-6ff6-47e5-a14a-994b35c222f0",
   "metadata": {},
   "source": [
    "# Embedding Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b83206-70e9-482e-9be5-6280e9732907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings(input, output):\n",
    "    with open(input, \"r\", encoding=\"utf8\") as f:\n",
    "        text = \" \".join(nltk.wordpunct_tokenize(f.read())).lower()\n",
    "    filename = input.split('.')[0] + \"_preprocessed.\" +  input.split('.')[1]\n",
    "    with open(filename, \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(text)\n",
    "    model = fasttext.train_unsupervised(filename, min_count=6, model='skipgram')\n",
    "    model.save_model(output + \".bin\")\n",
    "\n",
    "#make_embeddings('input/pd.txt', 'embeddings/pd')\n",
    "#make_embeddings('input/cr.txt', 'embeddings/cr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141dbc1-1d0e-43c9-ab29-811e902e2b30",
   "metadata": {},
   "source": [
    "# Embedding Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd9da1-4715-4b74-bfe9-ee498170fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python MUSE/supervised.py --src_lang \"cr\" --tgt_lang \"pd\" --emb_dim 100 --max_vocab -1 --n_refinement 20 --dico_train \"datasets/en_en_dict_train.txt\" --dico_eval \"datasets/en_en_dict_test.txt\" --src_emb  \"embeddings/cr.bin\" --tgt_emb  \"embeddings/pd.bin\" --cuda 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46026b7f-16f3-44ab-8393-b3fae6de217d",
   "metadata": {},
   "source": [
    "The aligned embedding models appear in the folder ./MUSE/dumped/debug/some_random_seed. Go to this folder and copy the files 'vectors-cr.txt' and 'vectors-pd.txt' into the folder 'embedding'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a27ef2-6d52-4e0d-bbb3-a65f49b5e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!LATEST_FOLDER=$(find MUSE/dumped/debug -type d -print0 | xargs -0 ls -td | head -n 1) && \\\n",
    "#cp \"$LATEST_FOLDER/vectors-cr.txt\" \"$LATEST_FOLDER/vectors-pd.txt\" embeddings/ && \\\n",
    "#echo \"Files copied to embeddings/\" || echo \"Error: Files not found in the latest folder.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1397061-d23c-4ad3-89ba-999ddc26e552",
   "metadata": {},
   "source": [
    "In the last step, we try to find new relationships between genes in the plant defense domain using the aligned models by using the seed relationships from the circadian rhythm domain. For each seed relationship, we obtain the 10 closest relationships according to the cosine similarity of genes in the plant defense domain for each seed relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e2ede-ddff-4f22-9b31-195fb72262a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "\n",
    "def get_gene_list():\n",
    "    with open(\"input/C_merged_PD_in_CR_trunc_clean_synonyms_B_genes_trunc.txt\", \"r\", encoding='utf8') as f:\n",
    "\n",
    "        vocab = defaultdict(int)\n",
    "        for line in f:\n",
    "            if len(line.strip()) > 0:\n",
    "                c = line.split()[1]\n",
    "                text = \" \".join(line.split()[2:]).strip()\n",
    "                text = \" \".join(nltk.wordpunct_tokenize(text)).lower()\n",
    "                if c == \"!pd\":\n",
    "                    words = text.split()\n",
    "                    for w in words:\n",
    "                        vocab[w] += 1\n",
    "\n",
    "        vocab = sorted(list(vocab.items()), reverse=True, key=lambda x: x[1])\n",
    "        vocab = [x[0] for x in vocab]\n",
    "        return set(vocab)\n",
    "\n",
    "\n",
    "def load_fasttext(emb_path, nmax=1000000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "def get_emb(word, emb, word2id):\n",
    "    avg_emb = []\n",
    "    for word_part in word.split():\n",
    "        word_emb = emb[word2id[word_part.lower()]].tolist()\n",
    "        avg_emb.append(word_emb)\n",
    "    avg = np.average(np.array(avg_emb), axis=0)\n",
    "    word_emb = avg\n",
    "    return word_emb\n",
    "\n",
    "def embeds_to_dict(emb, word2id):\n",
    "    dict = {}\n",
    "    words = list(word2id.items())\n",
    "    for w, id in words:\n",
    "        dict[w] = emb[id]\n",
    "    return dict\n",
    "\n",
    "def get_most_similar(word, word_emb, embeds, n = 10, word_list=[]):\n",
    "    neigh = []\n",
    "    items = list(embeds.items())\n",
    "    values = [v for k, v in items]\n",
    "    keys = [k for k, v in items]\n",
    "    #print(np.array(values).shape)\n",
    "    cs = cosine_similarity(word_emb.reshape(1, -1), np.array(values)).squeeze()\n",
    "    for i in range(len(keys)):\n",
    "        neigh.append((keys[i], cs[i]))\n",
    "    neigh = sorted(neigh, key=lambda x: x[1], reverse=True)\n",
    "    counter = 0\n",
    "    word_results = []\n",
    "    emb_results = []\n",
    "    for w, score in neigh:\n",
    "        if word.lower() not in w.lower() and w.lower() not in word.lower():\n",
    "            if len(word_list) == 0 or w in word_list:\n",
    "                if counter >= n:\n",
    "                    break\n",
    "                counter += 1\n",
    "                word_results.append((w, score))\n",
    "                emb_results.append(embeds[w])\n",
    "    return word_results, emb_results\n",
    "\n",
    "\n",
    "def get_all_relations(embeds, word2id):\n",
    "    gene_list = get_gene_list()\n",
    "    words = list(word2id.keys())\n",
    "    words = [x for x in words if x in gene_list]\n",
    "    diffs = {}\n",
    "    print(\"calculating all differences: \", len(words) * len(words))\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        for j in range(i + 1, len(words)):\n",
    "            word_1 = words[i]\n",
    "            word_2 = words[j]\n",
    "            emb_1 = embeds[word2id[word_1]]\n",
    "            emb_2 = embeds[word2id[word_2]]\n",
    "            diffs[word_1 + '-' + word_2] = emb_1 + emb_2\n",
    "            #diffs[word_2 + '-' + word_1] = emb_2 + emb_1\n",
    "            counter += 1\n",
    "            if counter % 1000000 == 0:\n",
    "                print(\"processing diff: \", counter)\n",
    "    print(\"Done\")\n",
    "    return diffs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_same_relations_in_domain_2(embeds_1, word2id_1, relations_1, embeds_2, word2id_2):\n",
    "\n",
    "    diffs = get_all_relations(embeds_2, word2id_2)\n",
    "\n",
    "    for rel in relations_1:\n",
    "        el1, el2 = rel\n",
    "        emb_1 = get_emb(el1, embeds_1, word2id_1)\n",
    "        emb_2 = get_emb(el2, embeds_1, word2id_1)\n",
    "        emb_rel = emb_1 + emb_2\n",
    "        word_res, emb_res = get_most_similar(el1 + '-' + el2.lower(), emb_rel, diffs, n=10, word_list=[])\n",
    "        print(\"Circadian rhythm: \", el1 + \" rel. \" + el2)\n",
    "        print(\"Most similar plant defense rel:\\n\")\n",
    "        print(\"rank\\trelation\\tcosine sim.\")\n",
    "        for idx, w in enumerate(word_res):\n",
    "            score = w[1]\n",
    "            w = w[0]\n",
    "            w = w.replace('-', ' rel. ')\n",
    "            print(str(idx + 1) + '.' + '\\t' + w + \"\\t{:.4f}\".format(score))\n",
    "        print('------------------------------------------')\n",
    "        print()\n",
    "\n",
    "\n",
    "def get_analogy(word_1, embeds_1, word2id_1, genes_1, word_2, embeds_2, word2id_2):\n",
    "\n",
    "    emb_1 = get_emb(word_1, embeds_1, word2id_1)\n",
    "    emb_2 = get_emb(word_2, embeds_2, word2id_2)\n",
    "    embeds_2 = embeds_to_dict(embeds_2, word2id_2)\n",
    "\n",
    "    for gene in genes_1:\n",
    "        emb_gene = get_emb(gene, embeds_1, word2id_1)\n",
    "        emb_result = emb_1 + emb_gene - emb_2\n",
    "        word_res, emb_res = get_most_similar(word_2.lower(), emb_result, embeds_2, n=10, word_list=get_gene_list())\n",
    "        print(\"Circadian rhythm domain: \", word_1 + ' rel. ' + gene.lower())\n",
    "        print(\"Most similar in plant defense domain:\\n\")\n",
    "        print('rank\\trelation\\tcosine sim.')\n",
    "        for idx, w in enumerate(word_res):\n",
    "            score = w[1]\n",
    "            w = w[0]\n",
    "            w = 'plant defense rel. ' + w\n",
    "            print(str(idx + 1) + '.' + '\\t' + w + \"\\t{:.4f}\".format(score))\n",
    "        print('------------------------------------------')\n",
    "        print()\n",
    "\n",
    "\n",
    "#Set our seed relations\n",
    "relations_1 = [['CCA1', 'PRR7'],\n",
    "               ['CCA1', 'PRR9'],\n",
    "               ['CCA1', 'PRR5'],\n",
    "               ['CCA1', 'TOC1'],\n",
    "               ['CCA1', 'ELF3'],\n",
    "               ['CCA1', 'ELF4'],\n",
    "               ['CCA1', 'LUX'],\n",
    "               ['LHY', 'PRR7'],\n",
    "               ['LHY', 'PRR9'],\n",
    "               ['LHY', 'PRR5'],\n",
    "               ['LHY', 'TOC1'],\n",
    "               ['LHY', 'ELF3'],\n",
    "               ['LHY', 'ELF4'],\n",
    "               ['LHY', 'LUX']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfc3be-ebdd-4567-82f6-f01b6acc4d77",
   "metadata": {},
   "source": [
    "To get exactly the same results as in the paper (reproducibility of results), use the files 'cr-aligned-original.vec' and 'pd-aligned-original.vec' in the embedding folder instead of the generated aligned embeddings. To do this, change the file names to the original embedding files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca25364-016d-441c-a7cc-a098b078b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to exactly reproduce the results in the paper, instead use the 'cr-aligned-original.vec' and 'pd-aligned-original.vec' files in the embedding folder.\n",
    "path_1 = 'embeddings/cr-aligned-original.vec'\n",
    "path_2 = 'embeddings/pd-aligned-original.vec'\n",
    "nmax = 500000  # maximum number of word embeddings to load\n",
    "embeds_1, id2word_1, word2id_1 = load_fasttext(path_1, nmax)\n",
    "embeds_2, id2word_2, word2id_2 = load_fasttext(path_2, nmax)\n",
    "get_same_relations_in_domain_2(embeds_1, word2id_1, relations_1, embeds_2, word2id_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b6068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
