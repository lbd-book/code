{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6349c747-4837-4743-91cc-78d82af89340",
   "metadata": {},
   "source": [
    "# Swanson's ABC closed discovery model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f59aedc-283e-4232-8cd4-805afedc8fb6",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "The main idea behind Swanson's work is that the two originally unconnected parts of the literature (C and A) can establish latent connections (through intermediate terms in B). These two pieces of literature have one or more b-terms in common, creating an indirect connection. Once these bridges are identified and validated, new knowledge can be generated.\n",
    "\n",
    "Swanson founded his discovery on the so-called ABC model, which is now widely regarded as a typical paradigm for LBD. The ABC model contains three concepts: the start concept ($c$), the intermediate concept ($b$), and the target concept ($a$). The LBD process begins with retrieving $c-b$ and $b-a$ relationships. Next, it combines associations with the same intermediates. Finally, it retrieves a list of $a-c$ relationships. If there is no prior mention of a particular $a-c$ connection, a hypothesis of a potential novel relationship between $a$ and $c$ concepts is conceived.\n",
    "\n",
    "<img src=\"img/closed_discovery.png\" alt=\"Swanson's ABC model\" width=\"300px\"/>\n",
    "\n",
    "In Swanson's scenario from 1986, literature domain $C$ refers to Raynaud's disease (set of papers mentioning Raynaud's disease), whereas domain $A$ represents the fish oil literature (set of papers mentioning fish oil)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d785cd-e048-4cb9-b05e-11d07414a666",
   "metadata": {},
   "source": [
    "[spaCy](https://spacy.io) is a powerful NLP library that offers a wide range of built-in functionalities for various NLP tasks. It has become an industry standard, with a vast ecosystem of extensions and plugins that can be easily integrated into custom components and machine learning workflows.\n",
    "\n",
    "In this notebook, we will use `en_core_web_md`, an English pipeline trained on written web text (blogs, news, comments). This pipeline includes vocabulary, syntax, and entities, and is optimized for CPU use. You can download the pipeline using the `spacy download` command.\n",
    "\n",
    "Note: If you are in a Jupyter notebook, you should use the `!` prefix to execute the following command. Make sure to restart your kernel or runtime after installation to make sure that the installed pipeline package can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d80cd5b-fb2d-4a19-b08a-abb969bc6205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required libraries for NLP and data analysis\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c46fb-6dbb-4c64-98d7-7e139187388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the small SpaCy English NLP model\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7231f777-d329-4881-ad86-15bcb63314cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SpaCy model into the `nlp` pipeline for NLP processing\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16f0233e-fd2b-4947-b376-5557872a9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets for the two domains (Raynaud's disease and Fish oil literature)\n",
    "# The datasets are stored as pipe-separated value (PSV) compressed files.\n",
    "# Only PMID and title columns are used for subsequent processing.\n",
    "c_df = pd.read_csv('./input/pmid_dp_ti_ab_raynaud_20241127.psv.gz', sep='|', names=['pmid', 'dp', 'ti', 'ab'], usecols=['pmid', 'ti'])\n",
    "a_df = pd.read_csv('./input/pmid_dp_ti_ab_fish_20241127.psv.gz', sep='|', names=['pmid', 'dp', 'ti', 'ab'], usecols=['pmid', 'ti'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c915086b-80e5-4b85-9e52-a515b7b91d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of documents in domain C: 1273\n",
      "No. of documents in domain A: 153\n"
     ]
    }
   ],
   "source": [
    "# Number of documents in domains\n",
    "print(f'No. of documents in domain C: {c_df.shape[0]}')\n",
    "print(f'No. of documents in domain A: {a_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba701266-f646-489f-b5df-2a78542e547b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "user_expressions": [
     {
      "expression": "c_df.loc[128, 'ti']",
      "result": {
       "data": {
        "text/plain": "\"[Surgery of thoracic outlet syndromes with Raynaud's disease. Report of 20 cases].\""
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "a_df.loc[126, 'ti']",
      "result": {
       "data": {
        "text/plain": "'[Nutritional lipidic factors of enzymatic phenobarbital induction in the rat. Effects of quantitative and qualitative composition of dietary lipids].'"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ]
   },
   "source": [
    "Before proceeding with further processing, we will remove the string '(author's transl)' from record titles originally published in non-English languages. For example, consider rows 126 (PMID 507701) and 128 (PMID 6783785) from the data frames `c_df` and `a_df` respectively. To simplyfy text processing we will employ `str.replace` method and remove '(author's transl)' string from titles where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bda24ff-72cd-4bff-92f7-99a69508baec",
   "metadata": {
    "user_expressions": [
     {
      "expression": "bla",
      "result": {
       "data": {
        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pmid</th>\n      <th>ti</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1483</td>\n      <td>The meaning of the Leydig cell in relation to ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6109</td>\n      <td>Raynaud's phenomenon as side effect of beta-bl...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8186</td>\n      <td>Letter: Raynaud's phenomenon as side effect of...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26828</td>\n      <td>[Raynaud's phenomenon after treatment with bet...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>37701</td>\n      <td>Vasospastic phenomena in patients treated with...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1268</th>\n      <td>20120572</td>\n      <td>Diagnosis of Raynaud's phenomenon in quarrymen...</td>\n    </tr>\n    <tr>\n      <th>1269</th>\n      <td>21283321</td>\n      <td>[Not Available].</td>\n    </tr>\n    <tr>\n      <th>1270</th>\n      <td>21308019</td>\n      <td>Skin reactions to cold.</td>\n    </tr>\n    <tr>\n      <th>1271</th>\n      <td>28211445</td>\n      <td>Linear Scleroderma with Unilateral Raynaud's P...</td>\n    </tr>\n    <tr>\n      <th>1272</th>\n      <td>29176123</td>\n      <td>Primary Raynaud's Disease- A Case Report.</td>\n    </tr>\n  </tbody>\n</table>\n<p>1273 rows Ã— 2 columns</p>\n</div>",
        "text/plain": "          pmid                                                 ti\n0         1483  The meaning of the Leydig cell in relation to ...\n1         6109  Raynaud's phenomenon as side effect of beta-bl...\n2         8186  Letter: Raynaud's phenomenon as side effect of...\n3        26828  [Raynaud's phenomenon after treatment with bet...\n4        37701  Vasospastic phenomena in patients treated with...\n...        ...                                                ...\n1268  20120572  Diagnosis of Raynaud's phenomenon in quarrymen...\n1269  21283321                                   [Not Available].\n1270  21308019                            Skin reactions to cold.\n1271  28211445  Linear Scleroderma with Unilateral Raynaud's P...\n1272  29176123          Primary Raynaud's Disease- A Case Report.\n\n[1273 rows x 2 columns]"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 128: [Surgery of thoracic outlet syndromes with Raynaud's disease. Report of 20 cases (author's transl)].\n",
      "A: 126: [Nutritional lipidic factors of enzymatic phenobarbital induction in the rat. Effects of quantitative and qualitative composition of dietary lipids (author's transl)].\n",
      "\n",
      "C: 128: [Surgery of thoracic outlet syndromes with Raynaud's disease. Report of 20 cases].\n",
      "A: 126: [Nutritional lipidic factors of enzymatic phenobarbital induction in the rat. Effects of quantitative and qualitative composition of dietary lipids].\n"
     ]
    }
   ],
   "source": [
    "# Inspecting an example before cleaning\n",
    "print(f\"C: 128: {c_df.loc[128, 'ti']}\")\n",
    "print(f\"A: 126: {a_df.loc[126, 'ti']}\")\n",
    "\n",
    "# Remove the translation marker from titles\n",
    "c_df['ti'] = c_df['ti'].str.replace(\" (author's transl)\", '')\n",
    "a_df['ti'] = a_df['ti'].str.replace(\" (author's transl)\", '')\n",
    "\n",
    "# Inspecting the titles again after cleaning\n",
    "print()\n",
    "print(f\"C: 128: {c_df.loc[128, 'ti']}\")\n",
    "print(f\"A: 126: {a_df.loc[126, 'ti']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd8d5dd1-589b-46c9-bea0-d81bdbe0047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default stopwords count: 326\n",
      "Updated stopwords count: 443\n"
     ]
    }
   ],
   "source": [
    "# Expand stop words with domain-specific PubMed stopwords\n",
    "\n",
    "# SpaCy has a built-in list of stopwords, but domain-specific datasets like PubMed\n",
    "# require additional stopwords to improve filtering during NLP tasks.\n",
    "\n",
    "# Get the current count of SpaCy's default stopwords\n",
    "print(f\"Default stopwords count: {len(nlp.Defaults.stop_words)}\")\n",
    "\n",
    "stopwords_pubmed = set(open('./input/stopwords_pubmed', 'r').read().splitlines())\n",
    "nlp.Defaults.stop_words |= stopwords_pubmed\n",
    "\n",
    "# Check the updated count of stopwords\n",
    "print(f\"Updated stopwords count: {len(nlp.Defaults.stop_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "150b271d-31fc-4a56-9a7d-ab2c0b6048fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# NLP tokenization and metadata extraction\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Define helper functions to extract token-level details using SpaCy.\n",
    "# The `extract_tokens_plus_meta` function extracts metadata for each token,\n",
    "# and `tidy_tokens` aggregates these details into a tidy format for analysis.\n",
    "\n",
    "def extract_tokens_plus_meta(doc:spacy.tokens.doc.Doc):\n",
    "    \"\"\"Extract tokens and metadata from individual spaCy doc.\"\"\"\n",
    "    return [\n",
    "        (i.text, i.i, i.lemma_, i.ent_type_, i.tag_, \n",
    "         i.dep_, i.pos_, i.is_stop, i.is_alpha, \n",
    "         i.is_digit, i.is_punct) for i in doc\n",
    "    ]\n",
    "\n",
    "def tidy_tokens(docs):\n",
    "    \"\"\"Extract tokens and metadata from list of spaCy docs.\"\"\"\n",
    "    \n",
    "    cols = [\n",
    "        \"pmid\", \"token\", \"token_order\", \"lemma\", \n",
    "        \"ent_type\", \"tag\", \"dep\", \"pos\", \"is_stop\", \n",
    "        \"is_alpha\", \"is_digit\", \"is_punct\"\n",
    "    ]\n",
    "    \n",
    "    ti2pmid_lst = docs.reindex(columns=['ti', 'pmid']).to_records(index=False,).tolist()\n",
    "    \n",
    "    meta_df = []\n",
    "    for ti, pmid in nlp.pipe(ti2pmid_lst, as_tuples=True):\n",
    "        meta = extract_tokens_plus_meta(ti)\n",
    "        meta = pd.DataFrame(meta)\n",
    "        meta.columns = cols[1:]\n",
    "        meta = meta.assign(pmid = pmid).loc[:, cols]\n",
    "        meta_df.append(meta)\n",
    "        \n",
    "    return pd.concat(meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1215e5b9-3806-4451-b739-7f4d4df00ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>token</th>\n",
       "      <th>token_order</th>\n",
       "      <th>lemma</th>\n",
       "      <th>ent_type</th>\n",
       "      <th>tag</th>\n",
       "      <th>dep</th>\n",
       "      <th>pos</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>is_digit</th>\n",
       "      <th>is_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91820</td>\n",
       "      <td>Fish</td>\n",
       "      <td>0</td>\n",
       "      <td>fish</td>\n",
       "      <td></td>\n",
       "      <td>NN</td>\n",
       "      <td>compound</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91820</td>\n",
       "      <td>oils</td>\n",
       "      <td>1</td>\n",
       "      <td>oil</td>\n",
       "      <td></td>\n",
       "      <td>NNS</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91820</td>\n",
       "      <td>,</td>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91820</td>\n",
       "      <td>prostaglandins</td>\n",
       "      <td>3</td>\n",
       "      <td>prostaglandin</td>\n",
       "      <td></td>\n",
       "      <td>NNS</td>\n",
       "      <td>conj</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91820</td>\n",
       "      <td>,</td>\n",
       "      <td>4</td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>,</td>\n",
       "      <td>punct</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pmid           token  token_order          lemma ent_type  tag       dep  \\\n",
       "0  91820            Fish            0           fish            NN  compound   \n",
       "1  91820            oils            1            oil           NNS      ROOT   \n",
       "2  91820               ,            2              ,             ,     punct   \n",
       "3  91820  prostaglandins            3  prostaglandin           NNS      conj   \n",
       "4  91820               ,            4              ,             ,     punct   \n",
       "\n",
       "     pos  is_stop  is_alpha  is_digit  is_punct  \n",
       "0   NOUN    False      True     False     False  \n",
       "1   NOUN    False      True     False     False  \n",
       "2  PUNCT    False     False     False      True  \n",
       "3   NOUN    False      True     False     False  \n",
       "4  PUNCT    False     False     False      True  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply tokenization and tidy the metadata for both datasets\n",
    "c_tbl = tidy_tokens(c_df)\n",
    "a_tbl = tidy_tokens(a_df)\n",
    "\n",
    "a_tbl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a91645c0-c573-46ac-bec5-95889e25c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of docs in domain C: 1273\n",
      "No. of docs in domain A: 153\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Filtering Tokens for Meaningful Features\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Filter out irrelevant tokens (stopwords, digits, and punctuation).\n",
    "# Aggregate tokens by their lemma for each PMID.\n",
    "c_lem_tbl = (c_tbl.query(\"is_stop == False & is_alpha == True & is_digit == False & is_punct == False\")\n",
    "             .sort_values(by=['pmid', 'token_order'])\n",
    "             .groupby('pmid')\n",
    "             .agg({'lemma': lambda x: ' '.join(x)})\n",
    "             .reset_index())\n",
    "\n",
    "a_lem_tbl = (a_tbl.query(\"is_stop == False & is_alpha == True & is_digit == False & is_punct == False\")\n",
    "             .sort_values(by=['pmid', 'token_order'])\n",
    "             .groupby('pmid')\n",
    "             .agg({'lemma': lambda x: ' '.join(x)})\n",
    "             .reset_index())\n",
    "\n",
    "# Check the number of documents after filtering\n",
    "print(f'No. of docs in domain C: {len(c_lem_tbl)}')\n",
    "print(f'No. of docs in domain A: {len(a_lem_tbl)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b06f549-7342-4fe0-967b-97648b59653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# TF-IDF analysis\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Apply TF-IDF vectorization to identify significant bigrams in the text.\n",
    "# N-gram range is set to (2, 2) for bigrams, and minimum document frequency is 2.\n",
    "c_vec = TfidfVectorizer(min_df=2, ngram_range=(2,2))\n",
    "c_ngrams = c_vec.fit_transform(c_lem_tbl['lemma'])\n",
    "c_cnt = c_ngrams.toarray().sum(axis=0)\n",
    "# Extract the ngrams and their frequencies into a DataFrame\n",
    "c_vocab = c_vec.vocabulary_\n",
    "c_df_ngram = pd.DataFrame(sorted([(k, c_cnt[i]) for k, i in c_vocab.items()]), columns=['ngram', 'freq'])\n",
    "\n",
    "# Repeat the process for the A-domain\n",
    "a_vec = TfidfVectorizer(min_df=2, ngram_range=(2,2))\n",
    "a_ngrams = a_vec.fit_transform(a_lem_tbl['lemma'])\n",
    "a_cnt = a_ngrams.toarray().sum(axis=0)\n",
    "a_vocab = a_vec.vocabulary_\n",
    "a_df_ngram = pd.DataFrame(sorted([(k, a_cnt[i]) for k, i in a_vocab.items()]), columns=['ngram', 'freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "007f5d7f-c2a9-4bda-add1-738afc153cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared b-terms: {'blood pressure', 'long term', 'platelet aggregation', 'blood viscosity', 'platelet function', 'myocardial infarction'}\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Identifying shared intermediate terms (b-terms)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Shared intermediate terms (b-terms) are identified as the intersection\n",
    "# of significant ngrams in both domains.\n",
    "c_ngram_lst = c_df_ngram['ngram'].to_list()\n",
    "a_ngram_lst = a_df_ngram['ngram'].to_list()\n",
    "b_terms = set(c_ngram_lst) & set(a_ngram_lst)\n",
    "\n",
    "print(f\"Shared b-terms: {b_terms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90d49261-9c8b-4683-8a17-af57cb74f5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   ngram    freq_c    freq_a  rank_c  rank_a\n",
      "1        blood viscosity  6.436829  1.029254     2.0     5.0\n",
      "3  myocardial infarction  2.787341  1.308833     4.0     3.0\n",
      "2              long term  8.008717  1.024821     1.0     6.0\n",
      "4   platelet aggregation  1.772796  1.472776     6.0     2.0\n",
      "0         blood pressure  3.736304  1.304103     3.0     4.0\n",
      "5      platelet function  1.796438  1.474065     5.0     1.0\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Borda count for rank aggregation\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Implement the Borda Count method to aggregate ranks of b-terms from\n",
    "# both C and A domains based on their frequencies.\n",
    "\n",
    "def borda_count(R, v=None):\n",
    "    \"\"\"\n",
    "    Borda count rank aggregation method.\n",
    "\n",
    "    Parameters:\n",
    "        R (numpy.ndarray): Matrix with rankings.\n",
    "        v (list or numpy.ndarray): Vector of votes for each ranking. Default is equal weights.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - r (numpy.ndarray): Vector with aggregated ranking.\n",
    "            - w (numpy.ndarray): Vector with weighted scores.\n",
    "    \"\"\"\n",
    "    m, n = R.shape\n",
    "    \n",
    "    # Assign equal votes if `v` is not provided\n",
    "    if v is None:\n",
    "        v = np.ones(n)\n",
    "    \n",
    "    # Initialize the weights matrix\n",
    "    W = np.zeros((m, n))\n",
    "    \n",
    "    # Populate the weights matrix based on rankings\n",
    "    for i in range(n):\n",
    "        r = R[:, i]\n",
    "        w = np.zeros(m)\n",
    "        # Assign scores from m to 1 based on descending order\n",
    "        w[np.argsort(-r)] = np.arange(m, 0, -1)\n",
    "        W[:, i] = w\n",
    "    \n",
    "    # Compute the weighted scores\n",
    "    w = W @ v\n",
    "    \n",
    "    # Aggregate the rankings\n",
    "    r = np.zeros(m, dtype=int)\n",
    "    r[np.argsort(-w)] = np.arange(1, m + 1)\n",
    "    \n",
    "    return r, w\n",
    "\n",
    "# Calculate the ranks and weights for shared b-terms\n",
    "c_df_rank = c_df_ngram.query('ngram in @b_terms')\n",
    "a_df_rank = a_df_ngram.query('ngram in @b_terms')\n",
    "\n",
    "rank_df = (pd.merge(c_df_rank, a_df_rank, on='ngram')\n",
    "           .set_axis(['ngram', 'freq_c', 'freq_a'], axis=1)\n",
    "           .assign(rank_c = lambda x: x.freq_c.rank(ascending=False))\n",
    "           .assign(rank_a = lambda x: x.freq_a.rank(ascending=False))\n",
    "          )\n",
    "\n",
    "# Perform Borda count rank aggregation\n",
    "R = np.array(rank_df[['rank_c', 'rank_a']])\n",
    "bc_r, bc_w = borda_count(R)\n",
    "\n",
    "# Sort the b-terms based on count output\n",
    "b_terms_rnk = rank_df.loc[bc_r - 1]\n",
    "print(b_terms_rnk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c4ee7-a801-42e5-b94e-cb8521b6f256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
